"""
GUCP-X å…¨ç»´ä¸€ä½“åŒ–é‡åŒ–ç³»ç»Ÿ (Supreme Unified System)
é¦–å¸­å…¨ç»´é‡åŒ–ç§‘å­¦å®¶: AI Architect
ç‰ˆæœ¬: SUPREME_GOLD_UNIFIED

æ ¸å¿ƒèƒ½åŠ›:
1. åŒæµæ£®æ—é¢„æµ‹ (Global + Positional)
2. ç‰©ç†åœºæ·±å±‚ç‰¹å¾æå– (Hurst, Entropy)
3. 20ç‚¹ä½æ ¸å¿ƒè£‚å˜æ‰©å±• (Smart Pool)
4. æ¨¡å‹å…¨è‡ªåŠ¨æŒä¹…åŒ–ä¸æœ‰æ•ˆæ€§éªŒè¯
5. æ»šåŠ¨å›æµ‹è‡ªåŠ¨åŒ–ç›ˆäºå¯¹è´¦
6. é¦–å¸­ç§‘å­¦å®¶çº§ä¸“ä¸šç ”æŠ¥ç”Ÿæˆ
"""

# [Supreme Fix] å¼ºåˆ¶é‡å®šå‘æ ‡å‡†è¾“å‡ºä¸º UTF-8,é˜²æ­¢æ§åˆ¶å°ä¹±ç 
import sys
import io
if sys.stdout.encoding != 'utf-8':
    sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding='utf-8')

import os
import sys
import time
import glob
import json
import logging
import warnings
import gc
import pickle
import shutil
import math
import random
import bisect
import re
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Tuple, Optional, Any, Union
from collections import defaultdict, Counter

import numpy as np
import pandas as pd
import yaml
import joblib
import psutil
from scipy import stats, signal
from sklearn.ensemble import RandomForestClassifier
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import TimeSeriesSplit
from sklearn.calibration import CalibratedClassifierCV
from sklearn.metrics import log_loss, roc_auc_score
from sklearn.neural_network import MLPClassifier
import matplotlib.pyplot as plt
import argparse
import hashlib
import torch
import torch.nn as nn
from torch.utils.data import DataLoader, TensorDataset

# æ–°å¢é«˜çº§ç®—æ³•åº“
import xgboost as xgb
import lightgbm as lgb
from statsmodels.tsa.arima.model import ARIMA
import optuna
from tqdm import tqdm

# ==========================================
# 0. ç³»ç»Ÿé…ç½®ä¸­å¿ƒ (Supreme Config)
# ==========================================

class SupremeConfig:
    """ç³»ç»Ÿçº§å…¨å±€é…ç½®å®¹å™¨ (æ”¯æŒå¤–éƒ¨ YAML åŠ¨æ€åŠ è½½)"""
    VERSION = "SUPREME_GOLD_UNIFIED"
    FEATURE_VERSION = "v5"  # å¤§å¹…æ‰©å±•ç‰¹å¾ç»´åº¦è‡³ 32 ç»´
    NUMBERS_PER_DRAW = 20
    TOTAL_NUMBERS = 80
    
    # è·¯å¾„é…ç½®
    # [Supreme Fix] ä½¿ç”¨åŠ¨æ€ç›¸å¯¹è·¯å¾„,å¢å¼ºç¯å¢ƒé€‚åº”æ€§
    BASE_DIR = Path(__file__).resolve().parent
    DATA_FILE = BASE_DIR / "data" / "kl8_history_final.txt"
    ORDER_FILE = BASE_DIR / "data" / "å¿«8å†å²å‡ºçƒé¡ºåº.txt"
    CACHE_DIR = BASE_DIR / "model_cache"
    REPORT_DIR = BASE_DIR / "data" / "reports"
    FEATURE_CACHE_DIR = BASE_DIR / "feature_cache"
    SELECT_DIR = BASE_DIR / "select"
    HISTORY_BASE_DIR = BASE_DIR / "data" / "history"
    CONFIG_FILE = BASE_DIR / "config.yaml"
    
    # å»ºæ¨¡å‚æ•° (é»˜è®¤å€¼)
    WINDOW_SIZE = 12
    RF_GLOBAL_PARAMS = {
        'n_estimators': 300,
        'max_depth': 15,
        'min_samples_split': 4,
        'n_jobs': 2,  # [Supreme Fix] Windows å®‰å…¨æ¨¡å¼,é¿å… -1 å¯¼è‡´çš„æ­»é”
        'random_state': 42,
        'class_weight': 'balanced'
    }
    
    RF_POS_PARAMS = {
        'n_estimators': 150,
        'max_depth': 10,
        'n_jobs': 2,
        'random_state': 42
    }
    
    MLP_PARAMS = {
        'hidden_layer_sizes': (128, 64, 32),
        'activation': 'relu',
        'solver': 'adam',
        'max_iter': 500,
        'random_state': 42,
        'early_stopping': True
    }

    # Stream D - TCN å‚æ•°
    TCN_PARAMS = {
        'seq_len': 30,
        'num_channels': [64, 64, 32],
        'kernel_size': 3,
        'dropout': 0.2,
        'learning_rate': 0.002,
        'epochs': 15
    }

    # Stream E - ARIMA å‚æ•°
    ARIMA_PARAMS = {
        'p': 2,
        'd': 1,
        'q': 1,
        'window': 50
    }

    # GBDT å‚æ•° (XGBoost & LightGBM)
    XGB_PARAMS = {
        'n_estimators': 500,  # å¢åŠ åŸºç¡€æ ‘é‡,é æ—©åœæ§åˆ¶
        'max_depth': 6,
        'learning_rate': 0.05,
        'subsample': 0.8,
        'colsample_bytree': 0.8,
        'random_state': 42,
        'verbosity': 0,
        'scale_pos_weight': 3.0  # å¤„ç† 1:3 çš„æ ·æœ¬ä¸å¹³è¡¡
    }
    
    LGB_PARAMS = {
        'n_estimators': 500,
        'max_depth': 6,
        'learning_rate': 0.05,
        'subsample': 0.8,
        'random_state': 42,
        'verbosity': -1,
        'class_weight': 'balanced'  # è‡ªåŠ¨å¤„ç†ä¸å¹³è¡¡
    }

    # å…³è”è§„åˆ™ & è·Ÿéšå¼ºåº¦å‚æ•°
    ASSOCIATION_PARAMS = {
        'min_support': 0.05,
        'min_confidence': 0.4,
        'analysis_window': 500
    }
    
    FOLLOWER_PARAMS = {
        'n_steps': 3,  # åˆ†ææœªæ¥ 3 æœŸçš„è·Ÿéš
        'min_strength': 0.1
    }

    # å¸‚åœºçŠ¶æ€é˜ˆå€¼ (å«ç†µå€¼)
    MARKET_REGIME_THRESHOLDS = {
        'stable_volatility': 0.04,
        'chaos_volatility': 0.07,
        'trend_slope': 2.5,
        'stable_entropy': 5.8,
        'chaos_entropy': 6.1
    }

    # èåˆæƒé‡ (Stream A+B+C, D, E, GBDT)
    FUSION_WEIGHTS = {
        'rf_mlp': 0.5,   # A+B+C
        'tcn': 0.2,      # D
        'arima': 0.1,    # E
        'gbdt': 0.2      # XGB + LGB
    }

    BACKTEST_PERIODS = 30
    
    # è‡ªåŠ¨è°ƒä¼˜å‚æ•°
    AUTO_TUNE_ENABLED = True
    AUTO_TUNE_TRIALS = 20
    AUTO_TUNE_PERIOD = 15
    VALIDATION_SIZE = 300  # å›ºå®šéªŒè¯é›†å¤§å° (æœ€æ–° 300 æœŸ)
    
    @staticmethod
    def load_external_config():
        """ä» config.yaml åŠ¨æ€åŒæ­¥å‚æ•°"""
        if not SupremeConfig.CONFIG_FILE.exists():
            return
        
        try:
            with open(SupremeConfig.CONFIG_FILE, 'r', encoding='utf-8') as f:
                cfg = yaml.safe_load(f)
                if not cfg:
                    return
                
                # 1. æœºå™¨å­¦ä¹ å‚æ•° (RF, MLP, TCN, ARIMA, GBDT)
                ml_cfg = cfg.get('ml', {})
                if 'rf_global_params' in ml_cfg:
                    SupremeConfig.RF_GLOBAL_PARAMS.update(ml_cfg['rf_global_params'])
                if 'mlp_params' in ml_cfg:
                    p = ml_cfg['mlp_params']
                    if 'hidden_layer_sizes' in p:
                        p['hidden_layer_sizes'] = tuple(p['hidden_layer_sizes'])
                    SupremeConfig.MLP_PARAMS.update(p)
                if 'tcn_params' in ml_cfg:
                    SupremeConfig.TCN_PARAMS.update(ml_cfg['tcn_params'])
                if 'arima_params' in ml_cfg:
                    SupremeConfig.ARIMA_PARAMS.update(ml_cfg['arima_params'])
                if 'xgb_params' in ml_cfg:
                    SupremeConfig.XGB_PARAMS.update(ml_cfg['xgb_params'])
                if 'lgb_params' in ml_cfg:
                    SupremeConfig.LGB_PARAMS.update(ml_cfg['lgb_params'])
                
                # 2. å…³è”è§„åˆ™å‚æ•°
                assoc_cfg = cfg.get('association', {})
                if assoc_cfg:
                    SupremeConfig.ASSOCIATION_PARAMS.update(assoc_cfg)

                # 3. å¸‚åœºçŠ¶æ€é˜ˆå€¼
                mkt_cfg = cfg.get('market', {})
                if 'regime_thresholds' in mkt_cfg:
                    SupremeConfig.MARKET_REGIME_THRESHOLDS.update(mkt_cfg['regime_thresholds'])

                # 4. èåˆæƒé‡
                pred_cfg = cfg.get('prediction', {})
                if 'fusion_weights' in pred_cfg:
                    SupremeConfig.FUSION_WEIGHTS.update(pred_cfg['fusion_weights'])

                # 5. è‡ªåŠ¨è°ƒä¼˜å‚æ•°
                tune_cfg = cfg.get('autotune', {})
                if 'enabled' in tune_cfg:
                    SupremeConfig.AUTO_TUNE_ENABLED = tune_cfg['enabled']
                if 'trials' in tune_cfg:
                    SupremeConfig.AUTO_TUNE_TRIALS = tune_cfg['trials']
                if 'period' in tune_cfg:
                    SupremeConfig.AUTO_TUNE_PERIOD = tune_cfg['period']
                if 'validation_size' in tune_cfg:
                    SupremeConfig.VALIDATION_SIZE = tune_cfg['validation_size']

                # 6. å›æµ‹å‚æ•°
                bt_cfg = cfg.get('backtest', {})
                if 'periods' in bt_cfg:
                    SupremeConfig.BACKTEST_PERIODS = bt_cfg['periods']
                if 'window_size' in bt_cfg:
                    SupremeConfig.WINDOW_SIZE = bt_cfg['window_size']
                
                logging.info("âš™ï¸ å¤–éƒ¨é…ç½®æ–‡ä»¶ config.yaml åŠ è½½æˆåŠŸ (å…¨å‚æ•°åŒæ­¥)")
        except Exception as e:
            logging.warning(f"âš ï¸ é…ç½®æ–‡ä»¶åŠ è½½å¤±è´¥,ä½¿ç”¨å†…ç½®é»˜è®¤å€¼: {e} ")

    @staticmethod
    def save_config():
        """å°†å½“å‰å†…å­˜ä¸­çš„é…ç½®æŒä¹…åŒ–å› config.yaml"""
        try:
            # å‡†å¤‡ç»“æ„åŒ–çš„é…ç½®æ•°æ®
            cfg_data = {
                "ml": {
                    "rf_global_params": SupremeConfig.RF_GLOBAL_PARAMS,
                    "mlp_params": {
                        **SupremeConfig.MLP_PARAMS,
                        "hidden_layer_sizes": list(SupremeConfig.MLP_PARAMS["hidden_layer_sizes"])
                    },
                    "tcn_params": SupremeConfig.TCN_PARAMS,
                    "arima_params": SupremeConfig.ARIMA_PARAMS,
                    "xgb_params": SupremeConfig.XGB_PARAMS,
                    "lgb_params": SupremeConfig.LGB_PARAMS
                },
                "association": SupremeConfig.ASSOCIATION_PARAMS,
                "market": {
                    "regime_thresholds": SupremeConfig.MARKET_REGIME_THRESHOLDS
                },
                "prediction": {
                    "fusion_weights": SupremeConfig.FUSION_WEIGHTS
                },
                "autotune": {
                    "enabled": SupremeConfig.AUTO_TUNE_ENABLED,
                    "trials": SupremeConfig.AUTO_TUNE_TRIALS,
                    "period": SupremeConfig.AUTO_TUNE_PERIOD,
                    "validation_size": SupremeConfig.VALIDATION_SIZE
                },
                "backtest": {
                    "periods": SupremeConfig.BACKTEST_PERIODS,
                    "window_size": SupremeConfig.WINDOW_SIZE
                }
            }
            
            with open(SupremeConfig.CONFIG_FILE, 'w', encoding='utf-8') as f:
                yaml.dump(cfg_data, f, allow_unicode=True, default_flow_style=False)
            logging.info(f"ğŸ’¾ æœ€ä½³å‚æ•°å·²æŒä¹…åŒ–è‡³ {SupremeConfig.CONFIG_FILE.name} ")
        except Exception as e:
            logging.error(f"âŒ é…ç½®æ–‡ä»¶ä¿å­˜å¤±è´¥: {e} ")

    @staticmethod
    def init_environment():
        """ç¯å¢ƒåˆå§‹åŒ–ä¸æ—¥å¿—è®¾ç½®"""
        # 1. åŸºç¡€ç›®å½•åˆ›å»º
        os.makedirs(SupremeConfig.CACHE_DIR, exist_ok=True)
        os.makedirs(SupremeConfig.REPORT_DIR, exist_ok=True)
        os.makedirs(SupremeConfig.FEATURE_CACHE_DIR, exist_ok=True)
        os.makedirs(SupremeConfig.SELECT_DIR, exist_ok=True)
        
        # 2. æ±‰åŒ–æ—¥å¿—æ ¼å¼
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s | %(levelname)-8s | %(name)-15s | %(message)s',
            handlers=[
                logging.StreamHandler(sys.stdout),
                logging.FileHandler(SupremeConfig.REPORT_DIR / "unified_system.log", encoding='utf-8')
            ]
        )
        
        # 3. åŠ è½½å¤–éƒ¨é…ç½®
        SupremeConfig.load_external_config()
        # ç¦ç”¨ matplotlib çš„å¹²æ‰°ä¿¡æ¯
        logging.getLogger('matplotlib').setLevel(logging.WARNING)
        # ç¦ç”¨æœªæ¥è­¦å‘Š
        warnings.filterwarnings('ignore')
        
        # ä¸­æ–‡æ”¯æŒ(Matplotlib)
        plt.rcParams['font.sans-serif'] = ['SimHei']  # Windows å¸¸ç”¨ä¸­æ–‡å­—ä½“
        plt.rcParams['axes.unicode_minus'] = False  # è§£å†³è´Ÿå·æ˜¾ç¤ºé—®é¢˜

# ==========================================
# 1. ç‰©ç†åœºå¼•æ“ (Physics Engine)
# ==========================================

class PhysicsEngine:
    """éçº¿æ€§åŠ¨åŠ›å­¦ç‰¹å¾æå–"""
    
    @staticmethod
    def calculate_hurst(series: np.ndarray) -> float:
        """è®¡ç®— R/S Hurst æŒ‡æ•°ä»¥è¡¡é‡æ—¶é—´åºåˆ—çš„è®°å¿†æ€§"""
        if len(series) < 20:
            return 0.5
        
        try:
            # ç®€åŒ–ç‰ˆ R/S åˆ†æ
            vals = series.astype(float)
            n = len(vals)
            # è®¡ç®—ç´¯ç§¯ç¦»å·®
            mean_val = np.mean(vals)
            y = np.cumsum(vals - mean_val)
            # è®¡ç®—æå·® R
            r = np.max(y) - np.min(y)
            # è®¡ç®—æ ‡å‡†å·® S
            s = np.std(vals)
            if s == 0:
                return 0.5
            # Hurst ä¼°è®¡ (ç®€å•è¿‘ä¼¼)
            hurst = math.log(r / s) / math.log(n)
            return np.clip(hurst, 0.0, 1.0)
        except Exception:
            return 0.5

    @staticmethod
    def calculate_metrics(history_subset: List[List[int]]) -> List[float]:
        """å¯¹å†å²ç‰‡æ®µæå–ç‰©ç†æŒ‡æ ‡ [ç†µ, å‡èƒ½, æ³¢åŠ¨ç‡, Hurst]"""
        flat = [n for row in history_subset for n in row]
        if not flat:
            return [3.0, 0.5, 0.5, 0.5]
        
        # 1. é¦™å†œç†µ (Shannon Entropy)
        counts = np.bincount(flat, minlength=81)[1:]
        probs = counts / (np.sum(counts) + 1e-10)
        ent = float(stats.entropy(probs))
        
        # 2. èƒ½é‡å‡å€¼ (Normalized Mean)
        mean_val = np.mean(flat) / 40.0
        
        # 3. æ³¢åŠ¨å¹…åº¦ (Normalized Volatility)
        vol = np.std(flat) / 23.0

        # 4. åºåˆ— Hurst æŒ‡æ•° (åŸºäºæ¯æœŸå’Œå€¼åºåˆ—)
        sums = np.array([sum(row) for row in history_subset])
        hurst = PhysicsEngine.calculate_hurst(sums)
        
        return [ent, mean_val, vol, hurst]

# ==========================================
# 2. æ•°æ®å¤„ç†å¼•æ“ (Data Engine)
# ==========================================

class DataEngine:
    """æ•°æ®å¯¹é½ä¸å®Œæ•´æ€§æ ¡éªŒ (å¢å¼ºå®¡è®¡ç‰ˆ)"""
    def __init__(self):
        self.logger = logging.getLogger("DataEngine")
        self.history: List[Dict] = []
        self.core_points: List[int] = []
        self.audit_log = []
        self._load_data()
        self._load_core_points()

    def _load_core_points(self):
        """ä»æœ€æ–°çš„å†å²ç›®å½•åŠ è½½æ ¸å¿ƒç‚¹ä½"""
        if not SupremeConfig.HISTORY_BASE_DIR.exists():
            return
            
        try:
            # è·å–æœ€æ–°çš„æ—¥æœŸç›®å½•
            history_dirs = [d for d in SupremeConfig.HISTORY_BASE_DIR.iterdir() if d.is_dir()]
            if not history_dirs:
                return
            
            latest_dir = max(history_dirs, key=lambda x: x.name)
            core_file = latest_dir / "core_points.txt"
            
            if core_file.exists():
                with open(core_file, 'r', encoding='utf-8-sig') as f:
                    content = f.read().strip()
                    if content:
                        # å…¼å®¹ç©ºæ ¼, é€—å·æˆ–çŸ­æ¨ªçº¿åˆ†éš”
                        nums = [int(n) for n in re.split(r'[,\s\-]+', content) if n]
                        self.core_points = sorted(list(set(nums)))
                        self.logger.info(f"âœ… å·²åŠ è½½æœ€æ–°æ ¸å¿ƒç‚¹ä½ ({latest_dir.name}): {self.core_points} ")
        except Exception as e:
            self.logger.warning(f"âš ï¸ æ ¸å¿ƒç‚¹ä½åŠ è½½å¤±è´¥: {e} ")

    def _load_data(self):
        """æ··åˆåŠ è½½æ ‡å‡†å†å²ä¸å‡ºçƒé¡ºåºæ•°æ®, å¹¶æ‰§è¡Œä¸¥æ ¼å®¡è®¡"""
        order_map = {}
        # A. åŠ è½½å‡ºçƒé¡ºåº
        if SupremeConfig.ORDER_FILE.exists():
            try:
                with open(SupremeConfig.ORDER_FILE, 'r', encoding='utf-8') as f:
                    lines = f.readlines()
                    header_skip = 1 if 'æœŸå·' in lines[0] else 0
                    for line in lines[header_skip:]:
                        parts = line.strip().split('\t')
                        if len(parts) >= 22:
                            pid = parts[0]
                            nums = [int(x) for x in parts[2:22]]
                            order_map[pid] = nums
            except Exception as e:
                self.logger.warning(f"é¡ºåºæ•°æ®åŠ è½½æç¤º: {e} ")

        # B. åŠ è½½æ ‡å‡†æ•°æ®
        temp_history = []
        if SupremeConfig.DATA_FILE.exists():
            try:
                with open(SupremeConfig.DATA_FILE, 'r', encoding='utf-8') as f:
                    for line in f:
                        if 'period:' not in line:
                            continue
                        meta = {}
                        for chunk in line.strip().split(','):
                            if ':' in chunk:
                                k, v = chunk.split(':', 1)
                                meta[k.strip()] = v.strip()
                        
                        pid = meta.get('period')
                        num_str = meta.get('numbers', '')
                        if pid and num_str:
                            sorted_nums = sorted([int(n) for n in num_str.replace('-', ' ').split()])
                            ordered_nums = order_map.get(pid, sorted_nums)
                            
                            # ä¸¥æ ¼æ ¡éªŒ:æ¯æœŸå¿…é¡»æ˜¯ 20 ä¸ªå·ç 
                            if len(sorted_nums) == 20:
                                temp_history.append({
                                    'period': pid,
                                    'date': meta.get('date', 'N/A'),
                                    'sorted': sorted_nums,
                                    'ordered': ordered_nums
                                })
            except Exception as e:
                self.logger.error(f"æ ‡å‡†æ•°æ®åŠ è½½å¼‚å¸¸: {e} ")
        
        temp_history.sort(key=lambda x: x['period'])
        
        # C. å¼ºåŒ–å®¡è®¡é€»è¾‘
        self.history = temp_history
        if len(self.history) > 1:
            # 1. ç¼ºå£æ£€æŸ¥
            pids = [int(x['period']) for x in self.history]
            gaps = []
            for i in range(len(pids)-1):
                if pids[i+1] - pids[i] > 1:
                    gaps.append(f"{pids[i]} -{pids[i+1]} ")
            
            if gaps:
                self.audit_log.append(f"âš ï¸ å‘ç°æ•°æ®ç¼ºå£: {', '.join(gaps)} ")
            else:
                self.audit_log.append("âœ… æœŸå·è¿ç»­æ€§æ ¡éªŒé€šè¿‡")
            
            # 2. é‡å¤æ£€æŸ¥
            if len(pids) != len(set(pids)):
                self.audit_log.append("âŒ è­¦å‘Š:å­˜åœ¨é‡å¤æœŸå·æ•°æ®")
            else:
                self.audit_log.append("âœ… æ•°æ®å”¯ä¸€æ€§æ ¡éªŒé€šè¿‡")

        self.logger.info(f"ğŸ“Š æ•°æ®å¼•æ“åˆå§‹åŒ–å®Œæ¯•: å…± {len(self.history)} æœŸè®°å½•")
        for log in self.audit_log:
            self.logger.info(f"  [å®¡è®¡] {log} ")

    def get_last_timestamp(self) -> float:
        """è·å–æ•°æ®æ–‡ä»¶çš„æœ€æ–°æ›´æ–°æ—¶é—´"""
        t1 = os.path.getmtime(SupremeConfig.DATA_FILE) if SupremeConfig.DATA_FILE.exists() else 0
        t2 = os.path.getmtime(SupremeConfig.ORDER_FILE) if SupremeConfig.ORDER_FILE.exists() else 0
        return max(t1, t2)

# ==========================================
# 3. æ ¸å¿ƒè®¡ç®—å¼•æ“ç¾¤ (Core Engines)
# ==========================================

class MarketEngine:
    """å¸‚åœºç¯å¢ƒæ„ŸçŸ¥æ¨¡å— (Market Regime)"""
    @staticmethod
    def calculate_entropy(history: List[Dict], window: int = 50) -> float:
        """è®¡ç®—é¦™å†œç†µ (Shannon Entropy) ä»¥è¯„ä¼°å·ç åˆ†å¸ƒçš„æ··æ²Œåº¦"""
        if len(history) < window:
            return 0.0
        recent_data = history[-window:]
        flat_list = [n for d in recent_data for n in d['sorted']]
        counts = Counter(flat_list)
        total = len(flat_list)
        # è®¡ç®—æ¦‚ç‡åˆ†å¸ƒçš„ç†µ
        probs = [count / total for count in counts.values()]
        entropy = -sum(p * math.log(p, 2) for p in probs)
        return round(entropy, 4)

    @staticmethod
    def analyze_regime(history: List[Dict]) -> Dict:
        """è¯†åˆ«ç›˜é¢çŠ¶æ€,å¹¶æ¨èæœ€ä¼˜çª—å£é•¿åº¦ (Adaptive Windowing)"""
        if len(history) < 20:
            return {"status": "æœªçŸ¥", "slope": 0.0, "volatility": 0.0, "entropy": 0.0, "recommended_window": 12}
        
        recent_sums = [sum(d['sorted']) for d in history[-20:]]
        # è®¡ç®—è¶‹åŠ¿æ–œç‡
        x = np.arange(len(recent_sums))
        slope, _, _, _, _ = stats.linregress(x, recent_sums)
        
        # è®¡ç®—æ³¢åŠ¨ç‡
        volatility = np.std(recent_sums) / np.mean(recent_sums)
        
        # è®¡ç®—ç†µå€¼
        entropy = MarketEngine.calculate_entropy(history)
        
        # åˆ¤å®šçŠ¶æ€ä¸æ¨èçª—å£
        if volatility < 0.04 and entropy < 5.8:  # ç†µå€¼ä½è¡¨ç¤ºåˆ†å¸ƒé›†ä¸­,è¾ƒç¨³å®š
            status = "âš–ï¸ Stable (Balanced)"
            recommended_window = 15  # ç¨³å®šæœŸä½¿ç”¨é•¿çª—å£,å¹³æ»‘å™ªå£°
        elif abs(slope) > 2.5:
            status = "ğŸ“ˆ Upward Trend" if slope > 0 else "ğŸ“‰ Downward Trend"
            recommended_window = 10  # è¶‹åŠ¿æœŸç¼©çŸ­çª—å£,æ•æ‰åŠ¨é‡
        elif volatility > 0.07 or entropy > 6.1:  # ç†µå€¼é«˜è¡¨ç¤ºåˆ†å¸ƒæ•£ä¹±,æ··æ²Œ
            status = "ğŸŒªï¸ Volatile (Chaos)"
            recommended_window = 8  # æ··æ²ŒæœŸä½¿ç”¨æçŸ­çª—å£,å¿«é€Ÿå“åº”å˜åŒ–
        else:
            status = "ğŸ”„ Mixed (Transition)"
            recommended_window = 12  # é»˜è®¤çª—å£
            
        return {
            "status": status,
            "slope": round(slope, 4),
            "volatility": round(volatility, 4),
            "entropy": entropy,
            "recommended_window": recommended_window
        }

class AssociationEngine:
    """å…³è”è§„åˆ™å¼•æ“ (Association Rules): è®¡ç®—å·ç é—´çš„æå‡åº¦ä¸ç½®ä¿¡åº¦"""
    def __init__(self):
        self.logger = logging.getLogger("Association")

    @staticmethod
    def mine_rules(history: List[Dict], min_support: float = 0.05, min_confidence: float = 0.4) -> List[Dict]:
        """æŒ–æ˜äºŒé˜¶å…³è”è§„åˆ™ (Pairwise Rules)"""
        total_draws = len(history)
        if total_draws < 100:
            return []
        
        # 1. è®¡æ•°
        item_counts = Counter()
        pair_counts = Counter()
        
        # ä»…åˆ†ææœ€è¿‘ 500 æœŸä»¥ä¿æŒç›¸å…³æ€§
        recent_history = history[-500:]
        recent_total = len(recent_history)
        
        for draw in recent_history:
            nums = sorted(draw['sorted'])
            for n in nums:
                item_counts[n] += 1
            for i in range(len(nums)):
                for j in range(i + 1, len(nums)):
                    pair_counts[(nums[i], nums[j])] += 1
        
        rules = []
        for (a, b), count in pair_counts.items():
            support_ab = count / recent_total
            if support_ab < min_support:
                continue
            
            support_a = item_counts[a] / recent_total
            support_b = item_counts[b] / recent_total
            
            # Confidence A -> B
            conf_a_b = support_ab / support_a
            # Confidence B -> A
            conf_b_a = support_ab / support_b
            
            # Lift (æå‡åº¦)
            lift = support_ab / (support_a * support_b)
            
            if conf_a_b >= min_confidence or conf_b_a >= min_confidence:
                rules.append({
                    "pair": f"{a:02d} -{b:02d} ",
                    "support": round(support_ab, 4),
                    "conf": round(max(conf_a_b, conf_b_a), 4),
                    "lift": round(lift, 4)
                })
        
        # æŒ‰æå‡åº¦æ’åº,å– Top 15
        return sorted(rules, key=lambda x: x['lift'], reverse=True)[:15]

class FollowerEngine:
    """è·Ÿéšå¼ºåº¦å¼•æ“ (Follower Strength): åˆ†æå·ç é—´çš„æ—¶åºè·Ÿéšå…³ç³»"""
    def __init__(self):
        self.logger = logging.getLogger("Follower")

    @staticmethod
    def analyze_followers(history: List[Dict], n_steps: int = 3, min_strength: float = 0.1) -> Dict[int, List[Dict]]:
        """åˆ†æå·ç  A å‡ºç°å,å·ç  B åœ¨æœªæ¥ N æœŸå†…å‡ºç°çš„è·Ÿéšå¼ºåº¦"""
        if len(history) < 200:
            return {}
        
        recent_history = history[-800:]
        total_draws = len(recent_history)
        
        # follower_counts[A][B] = count
        follower_counts = defaultdict(Counter)
        item_counts = Counter()
        
        for i in range(total_draws - n_steps):
            current_nums = recent_history[i]['sorted']
            for a in current_nums:
                item_counts[a] += 1
                # æ£€æŸ¥æœªæ¥ n_steps æœŸ
                future_nums = set()
                for step in range(1, n_steps + 1):
                    future_nums.update(recent_history[i + step]['sorted'])
                
                for b in future_nums:
                    follower_counts[a][b] += 1
        
        results = {}
        for a in range(1, 81):
            a_count = item_counts[a]
            if a_count == 0:
                continue
            
            followers = []
            for b, count in follower_counts[a].items():
                strength = count / a_count
                if strength >= min_strength:
                    followers.append({"num": b, "strength": round(strength, 4)})
            
            if followers:
                results[a] = sorted(followers, key=lambda x: x['strength'], reverse=True)[:10]
        
        return results

    @staticmethod
    def export_follower_stats(history: List[Dict], follower_rules: Dict[int, List[Dict]]):
        """å°†è·Ÿéšç»Ÿè®¡å’Œé¢‘æ¬¡å›¾è¡¨å›å†™åˆ°æœ€æ–°çš„å†å²ç›®å½•"""
        if not SupremeConfig.HISTORY_BASE_DIR.exists():
            return
            
        try:
            # è·å–æœ€æ–°çš„æ—¥æœŸç›®å½•
            history_dirs = [d for d in SupremeConfig.HISTORY_BASE_DIR.iterdir() if d.is_dir()]
            if not history_dirs:
                return
            
            latest_dir = max(history_dirs, key=lambda x: x.name)
            
            # 1. å›å†™è¯¦ç»†è·Ÿéšè§„åˆ™ (åŸæœ‰é€»è¾‘,æ”¹ä¸ºè¾“å‡ºåˆ° follow_stats.txt)
            stats_file = latest_dir / "follow_stats.txt"
            with open(stats_file, 'w', encoding='utf-8') as f:
                f.write(f"--- æ ¸å¿ƒè·Ÿéšè§„åˆ™ç»Ÿè®¡ ---\n")
                f.write(f"æ›´æ–°æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')} \n\n")
                for n in range(1, 81):
                    if n in follower_rules:
                        followers = follower_rules[n]
                        f_strs = [f"{item['num']:02d} ({item['strength']:.2f})" for item in followers]
                        f.write(f"{n:02d} -> {', '.join(f_strs)} \n")

            # 2. ç”Ÿæˆé¢‘æ¬¡å›¾è¡¨ (follow_10_chart, follow_25_chart, etc.)
            windows = {
                "10": 10,
                "25": 25,
                "50": 50,
                "2845": 2845  # ä»£è¡¨å¤§æ ·æœ¬æˆ–å…¨é‡
            }
            
            for name, win in windows.items():
                file_path = latest_dir / f"follow_{name}_chart.txt"
                # è®¡ç®—è¯¥çª—å£å†…çš„é¢‘æ¬¡
                subset = history[-win:] if len(history) >= win else history
                counts = Counter([n for d in subset for n in d['sorted']])
                
                with open(file_path, 'w', encoding='utf-8') as f:
                    f.write(f"âœ… {name} Game Chart({len(subset)} æœŸ)ä¼˜é€‰å·ç åˆ—è¡¨\n")
                    f.write(f"| å·ç  | å‘½ä¸­æ¬¡æ•°(HITS) |\n")
                    f.write(f"| :---: | :---: |\n")
                    # æŒ‰é¢‘æ¬¡ä»é«˜åˆ°ä½æ’åº
                    for n, count in counts.most_common(80):
                        # æ¨¡ä»¿åŸæœ‰æ ¼å¼,é«˜é¢‘å·åŠ æ˜Ÿå·
                        star = "*" if count >= (len(subset) * 0.3) else ""
                        f.write(f"| {n:02d}{star} | {count} |\n")
            
            logging.info(f"âœ… è·Ÿéšä¸é¢‘æ¬¡ç»Ÿè®¡å·²åŒæ­¥è‡³ {latest_dir.name} ")
        except Exception as e:
            logging.warning(f"âš ï¸ ç»Ÿè®¡å›å†™å¤±è´¥: {e} ")

class TCNBlock(nn.Module):
    """TCN æ®‹å·®å—: æ‰©å¼ å› æœå·ç§¯"""
    def __init__(self, in_channels, out_channels, kernel_size, dilation):
        super(TCNBlock, self).__init__()
        padding = (kernel_size - 1) * dilation
        self.conv1 = nn.Conv1d(in_channels, out_channels, kernel_size, 
                               padding=padding, dilation=dilation)
        self.relu = nn.ReLU()
        self.dropout = nn.Dropout(0.2)
        self.net = nn.Sequential(self.conv1, self.relu, self.dropout)
        self.res = nn.Conv1d(in_channels, out_channels, 1) if in_channels != out_channels else nn.Identity()

    def forward(self, x):
        out = self.net(x)
        # å› æœè£å‰ª (Causal Clipping)
        if self.conv1.padding[0] > 0:
            out = out[:, :, :-self.conv1.padding[0]]
        return self.relu(out + self.res(x))

class TCNModel(nn.Module):
    """æ—¶åºå·ç§¯ç½‘ç»œæ¨¡å‹: æ•è·é•¿ç¨‹ä¾èµ–"""
    def __init__(self, input_size, num_channels, kernel_size=3):
        super(TCNModel, self).__init__()
        layers = []
        for i in range(len(num_channels)):
            dilation = 2 ** i
            in_ch = input_size if i == 0 else num_channels[i-1]
            out_ch = num_channels[i]
            layers.append(TCNBlock(in_ch, out_ch, kernel_size, dilation))
        self.tcn = nn.Sequential(*layers)
        self.fc = nn.Linear(num_channels[-1], 80)

    def forward(self, x):
        # x: (batch, seq_len, features) -> (batch, features, seq_len)
        x = x.transpose(1, 2)
        y = self.tcn(x)
        return torch.sigmoid(self.fc(y[:, :, -1]))

class TCNEngine:
    """Stream D: æ—¶åºå·ç§¯ç½‘ç»œ (Temporal Convolutional Network)"""
    def __init__(self):
        self.logger = logging.getLogger("TCNEngine")
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        self.model = None
        self.model_path = SupremeConfig.CACHE_DIR / "global_tcn.pth"
        self.seq_len = 30  # é»˜è®¤è§‚å¯Ÿæœ€è¿‘ 30 æœŸ

    def prepare_data(self, history: List[Dict]):
        X, y = [], []
        # ä½¿ç”¨ 80 ç»´ one-hot ä½œä¸ºåŸå§‹è¾“å…¥
        for i in range(self.seq_len, len(history)):
            seq = []
            for j in range(i - self.seq_len, i):
                vec = np.zeros(80)
                for n in history[j]['sorted']:
                    vec[n-1] = 1
                seq.append(vec)
            X.append(seq)
            
            target = np.zeros(80)
            for n in history[i]['sorted']:
                target[n-1] = 1
            y.append(target)
        return torch.FloatTensor(np.array(X)), torch.FloatTensor(np.array(y))

    def train_or_load(self, history: List[Dict], data_time: float, mode: str = 'train', force: bool = False):
        # ç”ŸæˆåŸºäºæ¨¡å¼çš„æ¨¡å‹è·¯å¾„
        model_name = f"global_tcn_{mode}"
        model_path = SupremeConfig.CACHE_DIR / f"{model_name}.pth"

        if not force and model_path.exists() and os.path.getmtime(model_path) > data_time:
            try:
                self.model = TCNModel(80, SupremeConfig.TCN_PARAMS['num_channels']).to(self.device)
                self.model.load_state_dict(torch.load(model_path, map_location=self.device))
                self.logger.info(f"âœ… å·²åŠ è½½ Stream D: TCN Network ({mode})")
                return
            except Exception:
                pass

        self.logger.info(f"ğŸ§  æ­£åœ¨è®­ç»ƒ Stream D: TCN Neural Network ({mode})...")
        
        # ç»Ÿä¸€è®­ç»ƒçª—å£
        if mode == 'train':
            # åŸºç¡€æ ·æœ¬è®­ç»ƒ: ä½¿ç”¨å…¨é‡æ•°æ® - VALIDATION_SIZE
            train_history = history[:-SupremeConfig.VALIDATION_SIZE]
        else:
            train_history = history
            
        if len(train_history) < self.seq_len + 10:
            train_history = history[-1000:]

        full_X, full_y = self.prepare_data(train_history)
        split_idx = int(len(full_X) * 0.9)
        
        train_ds = TensorDataset(full_X[:split_idx], full_y[:split_idx])
        val_ds = TensorDataset(full_X[split_idx:], full_y[split_idx:])
        
        train_loader = DataLoader(train_ds, batch_size=32, shuffle=True)
        val_loader = DataLoader(val_ds, batch_size=32, shuffle=False)
        
        self.model = TCNModel(80, SupremeConfig.TCN_PARAMS['num_channels']).to(self.device)
        optimizer = torch.optim.Adam(self.model.parameters(), lr=SupremeConfig.TCN_PARAMS['learning_rate'])
        criterion = nn.BCELoss()
        
        best_val_loss = float('inf')
        patience = 5
        trigger_times = 0
        
        for epoch in range(SupremeConfig.TCN_PARAMS['epochs']):
            self.model.train()
            total_train_loss = 0
            for bx, by in train_loader:
                bx, by = bx.to(self.device), by.to(self.device)
                optimizer.zero_grad()
                out = self.model(bx)
                loss = criterion(out, by)
                loss.backward()
                optimizer.step()
                total_train_loss += loss.item()
            
            # éªŒè¯é›†è¯„ä¼°
            self.model.eval()
            total_val_loss = 0
            with torch.no_grad():
                for bx, by in val_loader:
                    bx, by = bx.to(self.device), by.to(self.device)
                    out = self.model(bx)
                    total_val_loss += criterion(out, by).item()
            
            avg_val_loss = total_val_loss / len(val_loader)
            if avg_val_loss < best_val_loss:
                best_val_loss = avg_val_loss
                torch.save(self.model.state_dict(), model_path)
                trigger_times = 0
            else:
                trigger_times += 1
                if trigger_times >= patience:
                    self.logger.info(f"Early stopping at epoch {epoch}")
                    break
        
        self.logger.info("ğŸš€ TCN å¼•æ“é‡è®­å®Œæˆ (å«æ—©åœä¸éªŒè¯)")

    def predict(self, history: List[Dict]) -> Dict[int, float]:
        if not self.model:
            return {i+1: 0.0 for i in range(80)}
        
        self.model.eval()
        with torch.no_grad():
            seq = []
            recent = history[-self.seq_len:]
            for d in recent:
                vec = np.zeros(80)
                for n in d['sorted']:
                    vec[n-1] = 1
                seq.append(vec)
            
            x = torch.FloatTensor(np.array([seq])).to(self.device)
            probs = self.model(x).cpu().numpy()[0]
        return {i+1: float(p) for i, p in enumerate(probs)}

class ARIMAEngine:
    """Stream E: ARIMA æ—¶åºæ¨¡å‹ (è¾…åŠ©å°æ ·æœ¬é¢„æµ‹)"""
    def __init__(self):
        self.logger = logging.getLogger("ARIMA")

    def predict(self, history: List[Dict]) -> Dict[int, float]:
        """å¯¹ 80 ä¸ªå·ç åˆ†åˆ«å»ºç«‹ ARIMA æ¨¡å‹è¿›è¡Œé¢„æµ‹"""
        if len(history) < 50:
            return {i: 0.0 for i in range(1, 81)}
        
        recent_window = history[-SupremeConfig.ARIMA_PARAMS['window']:]
        probs = {}
        
        # å‡†å¤‡æ¯ä¸ªå·ç çš„åºåˆ—
        # [ä¼˜åŒ–] æ·»åŠ  tqdm æ˜¾å¼è¿›åº¦æ¡,é¿å…ç”¨æˆ·ä»¥ä¸ºå¡æ­»
        iterator = tqdm(range(1, 81), desc="ğŸ“Š ARIMA Predicting", leave=False, unit="num")
        for n in iterator:
            series = [1 if n in d['sorted'] else 0 for d in recent_window]
            try:
                # ä½¿ç”¨ç®€å•çš„ ARIMA(2,1,1)
                with warnings.catch_warnings():
                    warnings.simplefilter("ignore")
                    model = ARIMA(series, order=(SupremeConfig.ARIMA_PARAMS['p'], 
                                                SupremeConfig.ARIMA_PARAMS['d'], 
                                                SupremeConfig.ARIMA_PARAMS['q']))
                    res = model.fit()
                    pred = res.forecast(steps=1)[0]
                    probs[n] = float(np.clip(pred, 0, 1))
            except Exception:
                probs[n] = 0.0
        return probs

class GBDTEngine:
    """GBDT å®¶æ—: XGBoost & LightGBM å¢å¼ºéçº¿æ€§æ‹Ÿåˆ"""
    def __init__(self):
        self.logger = logging.getLogger("GBDT")
        self.xgb_model = None
        self.lgb_model = None
        self.scaler = StandardScaler()

    def train(self, X: np.ndarray, y: np.ndarray):
        """è®­ç»ƒ XGBoost å’Œ LightGBM æ¨¡å‹ (å«æ—¶é—´åºåˆ—éªŒè¯é›†ä¸æ—©åœ)"""
        self.logger.info("ğŸŒ³ æ­£åœ¨è®­ç»ƒ GBDT å®¶æ— (XGBoost & LightGBM)...")
        
        # åˆ’åˆ†éªŒè¯é›† (æœ€å 10% æ•°æ®)
        split_idx = int(len(X) * 0.9)
        X_train, X_val = X[:split_idx], X[split_idx:]
        y_train, y_val = y[:split_idx], y[split_idx:]
        
        # XGBoost (é€‚é…æ–°ç‰ˆ API: early_stopping_rounds ç§»è‡³æ„é€ å‡½æ•°)
        xgb_params = SupremeConfig.XGB_PARAMS.copy()
        xgb_params['early_stopping_rounds'] = 20
        self.xgb_model = xgb.XGBClassifier(**xgb_params)
        self.xgb_model.fit(
            X_train, y_train,
            eval_set=[(X_val, y_val)],
            verbose=False
        )
        
        # LightGBM
        self.lgb_model = lgb.LGBMClassifier(**SupremeConfig.LGB_PARAMS)
        self.lgb_model.fit(
            X_train, y_train,
            eval_set=[(X_val, y_val)],
            eval_metric='binary_logloss',
            callbacks=[lgb.early_stopping(stopping_rounds=20, verbose=False)]
        )
        
    def predict_proba(self, X: np.ndarray) -> np.ndarray:
        """é›†æˆé¢„æµ‹æ¦‚ç‡"""
        if self.xgb_model is None or self.lgb_model is None:
            return np.zeros((X.shape[0], 2))
        
        p_xgb = self.xgb_model.predict_proba(X)
        p_lgb = self.lgb_model.predict_proba(X)
        # ç®€å•å¹³å‡èåˆ
        return (p_xgb + p_lgb) / 2.0

    def get_feature_importance(self) -> np.ndarray:
        """è·å– GBDT æ¨¡å‹çš„ç‰¹å¾é‡è¦æ€§"""
        if self.xgb_model is None or self.lgb_model is None:
            return np.zeros(32)
        
        # å½’ä¸€åŒ–åå¹³å‡
        xgb_imp = self.xgb_model.feature_importances_
        xgb_imp = xgb_imp / (np.sum(xgb_imp) + 1e-10)
        
        lgb_imp = self.lgb_model.feature_importances_
        lgb_imp = lgb_imp / (np.sum(lgb_imp) + 1e-10)
        
        return (xgb_imp + lgb_imp) / 2.0

    def mine_association_rules(self, history: List[Dict]) -> List[Dict]:
        """æŒ–æ˜å·ç é—´çš„å…³è”è§„åˆ™ (æ”¯æŒåº¦, ç½®ä¿¡åº¦, æå‡åº¦)"""
        if not history:
            return []
        
        # å‡†å¤‡äº‹åŠ¡æ•°æ®
        transactions = [set(d['sorted']) for d in history[-200:]]  # å–æœ€è¿‘ 200 æœŸ
        total = len(transactions)
        
        # 1. è®¡ç®—å•é¡¹æ”¯æŒåº¦
        support = defaultdict(int)
        for t in transactions:
            for n in t:
                support[n] += 1
        
        # 2. è®¡ç®—åŒé¡¹æ”¯æŒåº¦
        pair_support = defaultdict(int)
        for t in transactions:
            sorted_t = sorted(list(t))
            for i in range(len(sorted_t)):
                for j in range(i + 1, len(sorted_t)):
                    pair_support[(sorted_t[i], sorted_t[j])] += 1
                    
        # 3. è®¡ç®—æŒ‡æ ‡
        rules = []
        for (n1, n2), count in pair_support.items():
            s_pair = count / total
            if s_pair < SupremeConfig.ASSOCIATION_PARAMS['min_support']:
                continue
            
            s1 = support[n1] / total
            s2 = support[n2] / total
            
            # n1 -> n2
            conf = s_pair / s1
            lift = conf / s2
            
            if conf >= SupremeConfig.ASSOCIATION_PARAMS['min_confidence'] and lift >= SupremeConfig.ASSOCIATION_PARAMS.get('min_lift', 1.0):
                rules.append({
                    "pair": f"{n1:02d} -{n2:02d} ",
                    "support": round(s_pair, 4),
                    "conf": round(conf, 4),
                    "lift": round(lift, 4)
                })
                
        # æŒ‰æå‡åº¦æ’åº
        return sorted(rules, key=lambda x: x['lift'], reverse=True)

class AutoTuner:
    """è‡ªåŠ¨è°ƒä¼˜å¼•æ“: å‚æ•°, æ¨¡å‹ä¸å›æµ‹çš„æœ€ä¼˜åŒ–æ§åˆ¶"""
    def __init__(self, manager: 'SupremeManager'):
        self.manager = manager
        self.logger = logging.getLogger("AutoTuner")

    def objective(self, trial):
        """Optuna ä¼˜åŒ–ç›®æ ‡: åœ¨æ»šåŠ¨çª—å£å›æµ‹ä¸­å¯»æ‰¾æœ€å¤§å‘½ä¸­ç‡å‚æ•°"""
        # 1. å»ºè®®èåˆæƒé‡
        rf_mlp = trial.suggest_float("rf_mlp", 0.3, 0.7)
        gbdt = trial.suggest_float("gbdt", 0.1, 0.4)
        tcn = trial.suggest_float("tcn", 0.1, 0.3)
        arima = trial.suggest_float("arima", 0.05, 0.2)
        
        # å½’ä¸€åŒ–æƒé‡
        total = rf_mlp + gbdt + tcn + arima
        params = {
            "rf_mlp": rf_mlp / total,
            "gbdt": gbdt / total,
            "tcn": tcn / total,
            "arima": arima / total
        }
        
        # 2. å»ºè®®å…³é”®æ¨¡å‹å‚æ•°
        window = trial.suggest_int('window_size', 8, 20)
        
        # 3. å»ºè®® TCN ä¸ ARIMA çš„å†…éƒ¨å‚æ•° (æ·±åº¦è°ƒä¼˜)
        tcn_lr = trial.suggest_float("tcn_lr", 0.0005, 0.005, log=True)
        arima_p = trial.suggest_int("arima_p", 1, 3)
        
        # 4. å»ºè®® GBDT å‚æ•° (æ·±åº¦è°ƒä¼˜)
        xgb_lr = trial.suggest_float("xgb_lr", 0.01, 0.1)
        lgb_lr = trial.suggest_float("lgb_lr", 0.01, 0.1)
        
        # åº”ç”¨ä¸´æ—¶å‚æ•°è¿›è¡ŒéªŒè¯
        orig_weights = SupremeConfig.FUSION_WEIGHTS.copy()
        orig_window = SupremeConfig.WINDOW_SIZE
        orig_tcn_lr = SupremeConfig.TCN_PARAMS['learning_rate']
        orig_arima_p = SupremeConfig.ARIMA_PARAMS['p']
        orig_xgb_lr = SupremeConfig.XGB_PARAMS['learning_rate']
        orig_lgb_lr = SupremeConfig.LGB_PARAMS['learning_rate']
        
        SupremeConfig.FUSION_WEIGHTS.update(params)
        SupremeConfig.WINDOW_SIZE = window
        SupremeConfig.TCN_PARAMS['learning_rate'] = tcn_lr
        SupremeConfig.ARIMA_PARAMS['p'] = arima_p
        SupremeConfig.XGB_PARAMS['learning_rate'] = xgb_lr
        SupremeConfig.LGB_PARAMS['learning_rate'] = lgb_lr
        
        # [Windows ä¿®å¤] å¼ºåˆ¶ RF å•çº¿ç¨‹ä»¥é¿å…æ­»é”
        SupremeConfig.RF_GLOBAL_PARAMS['n_jobs'] = 1
        
        # 5. æ¨¡å‹å‡†å¤‡ (ä½¿ç”¨è®­ç»ƒé›†æ¨¡å¼ 'train')
        # è¿™å°†ç¡®ä¿è°ƒä¼˜æ˜¯åœ¨ (å…¨é‡æ•°æ® - 300æœŸ) ä¸Šè¿›è¡Œçš„
        history = self.manager.data_engine.history
        data_time = self.manager.data_engine.get_last_timestamp()
        
        self.manager.global_ml.train_or_load(history, data_time, window=window, mode='train')
        self.manager.pos_ml.train_or_load(history, data_time, mode='train')
        # TCN è®­ç»ƒè¾ƒæ…¢,é€šå¸¸ä¸å»ºè®®åœ¨æ¯è½® trial ä¸­é‡è®­,é™¤éå‚æ•°å˜åŒ–å¾ˆå¤§
        # self.manager.tcn_engine.train_or_load(history, data_time, mode='train')

        # 6. æ‰§è¡Œæ»šåŠ¨çª—å£å›æµ‹ (å›ºå®š VALIDATION_SIZE æœŸ)
        # æ³¨æ„:ä¸ºäº†æœ€å¤§åŒ–å‘½ä¸­ç‡,è¿™é‡Œéœ€è¦æ¨¡æ‹ŸçœŸå®çš„äº”æµèåˆé¢„æµ‹
        validator = AutoValidationEngine(
            self.manager.data_engine, 
            self.manager.global_ml, 
            self.manager.pos_ml
        )
        
        # åœ¨å›æµ‹å‰,å…ˆç”¨è®­ç»ƒé›† (History - VALIDATION_SIZE) é¢„çƒ­æ¨¡å‹
        # è¿™æ ·è°ƒä¼˜çš„æ˜¯é’ˆå¯¹"æœªçŸ¥"æ•°æ®çš„æ³›åŒ–èƒ½åŠ›
        history = self.manager.data_engine.history
        split_idx = len(history) - SupremeConfig.VALIDATION_SIZE
        train_history = history[:split_idx]
        
        # é¢„è®¡ç®—å›æµ‹æœŸé—´çš„æ‰€æœ‰ TCN å’Œ ARIMA é¢„æµ‹,é¿å…é‡å¤è®¡ç®—
        tcn_probs_all = {}
        arima_probs_all = {}
        
        for i in range(split_idx, len(history)):
            known_history = history[:i]
            # [ä¼˜åŒ–] TCN å’Œ ARIMA é¢„æµ‹è€—æ—¶è¾ƒé•¿,å¢åŠ æ—¥å¿—
            if i % 5 == 0: 
                self.logger.info(f"....Pre-calculating Period {history[i]['period']} (TCN/ARIMA)")
            tcn_probs_all[i] = self.manager.tcn_engine.predict(known_history)
            arima_probs_all[i] = self.manager.arima_engine.predict(known_history)

        # è¿è¡Œå›æµ‹å¹¶è·å–å¹³å‡å‘½ä¸­ç‡
        avg_hits = validator.run_backtest_full(
            periods=SupremeConfig.VALIDATION_SIZE, 
            params=params, 
            tcn_probs_stream=tcn_probs_all,
            arima_probs_stream=arima_probs_all
        )
        
        # æ¢å¤åŸå§‹å‚æ•°
        SupremeConfig.FUSION_WEIGHTS = orig_weights
        SupremeConfig.WINDOW_SIZE = orig_window
        SupremeConfig.TCN_PARAMS['learning_rate'] = orig_tcn_lr
        SupremeConfig.ARIMA_PARAMS['p'] = orig_arima_p
        SupremeConfig.XGB_PARAMS['learning_rate'] = orig_xgb_lr
        SupremeConfig.LGB_PARAMS['learning_rate'] = orig_lgb_lr
        # [Windows ä¿®å¤] æ¢å¤å¹¶è¡Œ
        SupremeConfig.RF_GLOBAL_PARAMS['n_jobs'] = -1
        
        return avg_hits

    def tune(self):
        """æ‰§è¡Œå…¨è‡ªåŠ¨è°ƒä¼˜å¹¶åº”ç”¨æœ€ä½³é…ç½®"""
        if not SupremeConfig.AUTO_TUNE_ENABLED:
            return
            
        self.logger.info(f"ğŸ¯ å¯åŠ¨å…¨è‡ªåŠ¨å‚æ•°è°ƒä¼˜ (Optuna, Trials={SupremeConfig.AUTO_TUNE_TRIALS})...")
        try:
            # å¢åŠ å¹¶è¡Œè°ƒä¼˜æ”¯æŒ (å¦‚æœèµ„æºå…è®¸)
            # [ä¼˜åŒ–] ä½¿ç”¨ MedianPruner æå‰å‰ªææ— æ•ˆçš„ Trial
            study = optuna.create_study(direction='maximize', pruner=optuna.pruners.MedianPruner())
            
            # [ä¼˜åŒ–] ä½¿ç”¨ tqdm æ˜¾ç¤ºè°ƒä¼˜è¿›åº¦, æ‰‹åŠ¨è¿­ä»£ä¼˜åŒ–
            pbar = tqdm(range(SupremeConfig.AUTO_TUNE_TRIALS), desc="ğŸ”¥ AutoTuning", unit="trial")
            for _ in pbar:
                study.optimize(self.objective, n_trials=1, n_jobs=1)  # å¼ºåˆ¶å•è¿›ç¨‹ä»¥é˜² Windows æ­»é”
                pbar.set_postfix({"best_score": f"{study.best_value:.4f}"})
            
            pbar.close()
            
            best_params = study.best_params
            self.logger.info(f"ğŸ† è°ƒä¼˜å®Œæˆ! æœ€ä½³å¹³å‡å‘½ä¸­: {study.best_value:.4f}")
            
            # 1. åº”ç”¨æœ€ä½³æƒé‡å¹¶å½’ä¸€åŒ–
            w_keys = ['rf_mlp', 'gbdt', 'tcn', 'arima']
            best_weights = {k: best_params[k] for k in w_keys if k in best_params}
            if best_weights:
                total_w = sum(best_weights.values())
                final_weights = {k: v/total_w for k, v in best_weights.items()}
                SupremeConfig.FUSION_WEIGHTS.update(final_weights)
                self.logger.info(f"ğŸ“ æœ€ä½³æƒé‡å·²åº”ç”¨: {final_weights}")
            
            # 2. åº”ç”¨æœ€ä½³æ¨¡å‹å‚æ•°
            if 'window_size' in best_params:
                SupremeConfig.WINDOW_SIZE = best_params['window_size']
                self.logger.info(f"ğŸ“ æœ€ä½³çª—å£å·²åº”ç”¨: {best_params['window_size']}")
            
            if 'tcn_lr' in best_params:
                SupremeConfig.TCN_PARAMS['learning_rate'] = best_params['tcn_lr']
            if 'arima_p' in best_params:
                SupremeConfig.ARIMA_PARAMS['p'] = best_params['arima_p']
            if 'xgb_lr' in best_params:
                SupremeConfig.XGB_PARAMS['learning_rate'] = best_params['xgb_lr']
            if 'lgb_lr' in best_params:
                SupremeConfig.LGB_PARAMS['learning_rate'] = best_params['lgb_lr']
                
            # 3. è®°å½•è°ƒä¼˜å†å²ä»¥ä¾¿è‡ªåŠ¨åŒ–åˆ†æè´¡çŒ®åº¦
            self._log_tuner_history(best_params, study.best_value)
            
            # 4. æŒä¹…åŒ–æœ€ä½³å‚æ•°åˆ°ç£ç›˜
            SupremeConfig.save_config()
                
        except Exception as e:
            self.logger.error(f"âŒ è‡ªåŠ¨è°ƒä¼˜è¿‡ç¨‹å‡ºé”™: {e}")
            import traceback
            self.logger.error(traceback.format_exc())

    def _log_tuner_history(self, best_params: Dict, best_value: float):
        """è®°å½•è°ƒä¼˜å†å²åˆ°æœ¬åœ° JSON æ–‡ä»¶"""
        history_path = SupremeConfig.BASE_DIR / "data" / "tuner_history.json"
        history = []
        if history_path.exists():
            try:
                with open(history_path, 'r', encoding='utf-8') as f:
                    history = json.load(f)
            except Exception:
                pass
            
        new_entry = {
            "timestamp": datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
            "best_value": round(best_value, 4),
            "weights": {k: round(best_params[k], 4) for k in ['rf_mlp', 'gbdt', 'tcn', 'arima'] if k in best_params},
            "window_size": best_params.get('window_size')
        }
        
        # å½’ä¸€åŒ–æƒé‡è®°å½•
        if "weights" in new_entry:
            total_w = sum(new_entry["weights"].values())
            if total_w > 0:
                new_entry["weights"] = {k: round(v/total_w, 4) for k, v in new_entry["weights"].items()}
        
        history.append(new_entry)
        # åªä¿ç•™æœ€è¿‘ 50 æ¬¡è®°å½•
        history = history[-50:]
        
        with open(history_path, 'w', encoding='utf-8') as f:
            json.dump(history, f, indent=4, ensure_ascii=False)
        self.logger.info(f"ğŸ“Š è°ƒä¼˜å†å²å·²æ›´æ–°è‡³ {history_path.name}")

class MLEngine:
    """Stream A: å…¨å±€æ„ŸçŸ¥æ£®æ— (Global ML Model) + Stream C: æ·±åº¦å­¦ä¹ ç¥ç»å…ƒ (Deep Learning) + GBDT (XGBoost/LightGBM)"""
    def __init__(self):
        self.logger = logging.getLogger("GlobalML")
        self.model_rf = None
        self.model_mlp = None
        self.model_gbdt = GBDTEngine()  # é›†æˆ GBDT å®¶æ—
        self.scaler = StandardScaler()
        self.feature_importances = []  # åˆå§‹åŒ–ç‰¹å¾é‡è¦æ€§åˆ—è¡¨
        version_tag = datetime.now().strftime('%Y%m%d')
        self.model_path = SupremeConfig.CACHE_DIR / f"global_ensemble_{version_tag}.joblib"

    def _get_cache_path(self, history: List[Dict], window: int, mode: str) -> Path:
        """ç”ŸæˆåŸºäºæ•°æ®ç‰¹å¾çš„ MD5 ç¼“å­˜è·¯å¾„"""
        content = f"{len(history)}_{window}_{mode}_{SupremeConfig.FEATURE_VERSION}" 
        if history:
            content += f"_{history[-1]['period']}"
        h_md5 = hashlib.md5(content.encode()).hexdigest()[:12]
        return SupremeConfig.FEATURE_CACHE_DIR / f"feat_global_{h_md5}.pkl"

    def _calculate_follower_matrix(self, history: List[Dict]) -> np.ndarray:
        """è®¡ç®—å·ç è·Ÿéšæ¦‚ç‡çŸ©é˜µ (Row: å‰æœŸå·ç , Col: åæœŸå·ç )"""
        matrix = np.zeros((81, 81))
        counts = np.zeros(81)
        for i in range(len(history) - 1):
            prev_nums = history[i]['sorted']
            curr_nums = history[i+1]['sorted']
            for p in prev_nums:
                counts[p] += 1
                for c in curr_nums:
                    matrix[p][c] += 1
        
        # å½’ä¸€åŒ–ä¸ºæ¦‚ç‡
        for i in range(1, 81):
            if counts[i] > 0:
                matrix[i] /= counts[i]
        return matrix

    def construct_features(self, history: List[Dict], window: int = 12, mode: str = 'train') -> Tuple[np.ndarray, np.ndarray]:
        """æ„å»ºæ·±åº¦ç‰¹å¾çŸ©é˜µ (é›†æˆè·¨æœŸç›¸å…³æ€§, é—æ¼è¡°å‡åŠè‡ªé€‚åº”çª—å£)"""
        cache_path = self._get_cache_path(history, window, mode)
        
        data_mtime = max(
            os.path.getmtime(SupremeConfig.DATA_FILE) if SupremeConfig.DATA_FILE.exists() else 0,
            os.path.getmtime(SupremeConfig.ORDER_FILE) if SupremeConfig.ORDER_FILE.exists() else 0
        )
        
        if cache_path.exists() and cache_path.stat().st_mtime > data_mtime:
            try:
                with open(cache_path, 'rb') as f:
                    self.logger.info(f"ğŸ’¾ åŠ è½½ç‰¹å¾ç¼“å­˜: {cache_path.name} (Window={window})")
                    return pickle.load(f)
            except Exception:
                pass

        self.logger.info(f"âš™ï¸ æ„é€ ç‰¹å¾ (Window={window}, Mode={mode})...")
        X, y = [], []
        total_len = len(history)
        
        # é¢„è®¡ç®—å…¨å±€å‡ºç°ä½ç½®ä¸è·ŸéšçŸ©é˜µ
        appearances = defaultdict(list)
        for idx, item in enumerate(history):
            for n in item['sorted']:
                appearances[n].append(idx)
        
        # è·ŸéšçŸ©é˜µè®¡ç®—
        if mode == 'train':
            train_end = max(window + 5, total_len - SupremeConfig.VALIDATION_SIZE)
            follower_matrix = self._calculate_follower_matrix(history[:train_end-1])
        else:
            follower_matrix = self._calculate_follower_matrix(history)
        
        if mode == 'predict':
            loop_range = [total_len]
        elif mode == 'validate':
            start_idx = max(window + 2, total_len - SupremeConfig.VALIDATION_SIZE)
            loop_range = range(start_idx, total_len)
        elif mode == 'production':
            loop_range = range(window + 2, total_len)
        else:
            end_idx = max(window + 5, total_len - SupremeConfig.VALIDATION_SIZE)
            loop_range = range(window + 2, end_idx)

        for i in loop_range:
            slice_data = history[i-window : i]
            slice_sorted = [d['sorted'] for d in slice_data]
            phy_feats = PhysicsEngine.calculate_metrics(slice_sorted)
            
            w2, w4 = window * 2, window * 4
            slice_w2 = history[max(0, i-w2) : i]
            slice_w4 = history[max(0, i-w4) : i]
            
            counts_w1 = Counter([n for row in slice_sorted for n in row])
            counts_w2 = Counter([n for row in [d['sorted'] for d in slice_w2] for n in row])
            counts_w4 = Counter([n for row in [d['sorted'] for d in slice_w4] for n in row])
            tail_counts = Counter([n % 10 for n in [n for row in slice_sorted for n in row]])
            
            last_sorted = history[i-1]['sorted'] if i > 0 else []
            last_set = set(last_sorted)
            last_neighbor_set = set()
            for ln in last_set:
                last_neighbor_set.add(ln-1)
                last_neighbor_set.add(ln+1)
            
            before_last_set = set(history[i-2]['sorted']) if i > 1 else set()
            last_3_sets = [set(history[i-j]['sorted']) for j in range(1, min(4, i+1))]
            
            target_set = set(history[i]['sorted']) if i < total_len else set()

            for n in range(1, 81):
                f1 = counts_w1.get(n, 0) / window
                f2 = counts_w2.get(n, 0) / (len(slice_w2) or 1)
                f4 = counts_w4.get(n, 0) / (len(slice_w4) or 1)
                
                idx_pos = bisect.bisect_left(appearances[n], i)
                gap = i - 1 - appearances[n][idx_pos-1] if idx_pos > 0 else window
                avg_gap = total_len / (len(appearances[n]) or 1)
                
                decay = math.exp(-0.15 * gap)
                is_repeat = 1.0 if n in last_set else 0.0
                is_neighbor = 1.0 if n in last_neighbor_set else 0.0
                is_jump = 1.0 if (n in before_last_set and n not in last_set) else 0.0
                
                follower_score = 0.0
                if last_sorted:
                    follower_score = np.mean([follower_matrix[prev_n][n] for prev_n in last_sorted])
                neighbor_heat = (counts_w1.get(n-1, 0) + counts_w1.get(n+1, 0)) / (2 * window)
                tail_heat = tail_counts.get(n % 10, 0) / (window * 2)
                
                hit_series = [1 if n in set(h['sorted']) else 0 for h in slice_data]
                std_w1 = np.std(hit_series)
                last_1_hit = 1.0 if n in last_3_sets[0] else 0.0 if last_3_sets else 0.0
                last_3_hits = sum(1 for s in last_3_sets if n in s)
                
                is_even = 1.0 if n % 2 == 0 else 0.0
                is_prime = 1.0 if n in [2, 3, 5, 7, 11, 13, 17, 19, 23, 29, 31, 37, 41, 43, 47, 53, 59, 61, 67, 71, 73, 79] else 0.0
                is_big = 1.0 if n > 40 else 0.0
                
                inter_1 = f1 * decay
                inter_2 = follower_score * neighbor_heat
                inter_3 = is_prime * f1
                inter_4 = is_big * math.log(gap + 1)

                row = [
                    n/80.0, f1, f2, f4,
                    math.log(gap + 1), math.log(avg_gap + 1), decay,
                    is_repeat, is_neighbor, is_jump, follower_score,
                    neighbor_heat, tail_heat, std_w1,
                    last_1_hit, last_3_hits,
                    is_even, is_prime, is_big, n%3, n%5,
                    f1/(f2+0.001), f2/(f4+0.001),
                    inter_1, inter_2, inter_3, inter_4,
                    np.mean(hit_series[-5:]) if len(hit_series) >= 5 else f1
                ] + phy_feats
                
                X.append(row)
                if mode in ['train', 'validate', 'production']:
                    y.append(1 if n in target_set else 0)
                    
        res = (np.array(X, dtype=np.float32), np.array(y, dtype=np.int8))
        try:
            with open(cache_path, 'wb') as f:
                pickle.dump(res, f)
        except Exception:
            pass
            
        return res

    def train_or_load(self, history: List[Dict], data_time: float, window: int = 12, mode: str = 'train', force: bool = False):
        """åŠ è½½æœ‰æ•ˆæ¨¡å‹æˆ–é‡è®­ (æ”¯æŒå¤šæµèåˆ:RF + MLP + GBDT)"""
        # ç”ŸæˆåŸºäºæ¨¡å¼çš„æ¨¡å‹è·¯å¾„
        model_name = f"global_ensemble_{mode}_{window}"
        version_tag = datetime.now().strftime('%Y%m%d')
        model_path = SupremeConfig.CACHE_DIR / f"{model_name}_{version_tag}.joblib"

        if not force and model_path.exists():
            model_time = os.path.getmtime(model_path)
            if model_time > data_time:
                try:
                    self.model_rf, self.model_mlp, self.model_gbdt, self.scaler, self.feature_importances = joblib.load(model_path)
                    self.logger.info(f"âœ… å·²åŠ è½½ Global Ensemble ({mode}, Window={window})")
                    return
                except Exception:
                    pass

        X, y = self.construct_features(history, window=window, mode=mode)
        self.scaler.fit(X)
        X_scaled = self.scaler.transform(X)
        
        # 1. è®­ç»ƒéšæœºæ£®æ— (Stream A)
        self.logger.info("ğŸ“¡ æ­£åœ¨è®­ç»ƒ Stream A: Global Random Forest...")
        base_rf = RandomForestClassifier(**SupremeConfig.RF_GLOBAL_PARAMS)
        self.model_rf = CalibratedClassifierCV(base_rf, method='isotonic', cv=3)
        self.model_rf.fit(X_scaled, y)
        
        # 2. è®­ç»ƒå¤šå±‚æ„ŸçŸ¥æœº (Stream C: Deep Learning)
        self.logger.info("ğŸ§  æ­£åœ¨è®­ç»ƒ Stream C: MLP Neural Network...")
        self.model_mlp = MLPClassifier(**SupremeConfig.MLP_PARAMS)
        self.model_mlp.fit(X_scaled, y)

        # 3. è®­ç»ƒ GBDT å®¶æ— (XGBoost + LightGBM)
        self.model_gbdt.train(X_scaled, y)
        
        # 4. è®¡ç®—ç‰¹å¾é‡è¦æ€§ (åŸºäº RF å’Œ GBDT çš„èåˆ)
        # ç‰¹å¾åç§°å¯¹åº”å…³ç³» (32 ç»´æ‰©å±•)
        feature_names = [
            "Num_Norm", "Freq_W1", "Freq_W2", "Freq_W4",
            "Gap_Log", "Avg_Gap_Log", "Decay",
            "Is_Repeat", "Is_Neighbor", "Is_Jump", "Follower_Score",
            "Neighbor_Heat", "Tail_Heat", "Std_W1",
            "Last_1_Hit", "Last_3_Hits",
            "Is_Even", "Is_Prime", "Is_Big", "Mod_3", "Mod_5",
            "Trend_W12", "Trend_W24",
            "Inter_1", "Inter_2", "Inter_3", "Inter_4",
            "Recent_5_Avg",
            "Entropy", "Mean_Energy", "Volatility", "Hurst"
        ]
        
        # æå– RF é‡è¦æ€§ (é€‚é… CalibratedClassifierCV ç»“æ„)
        try:
            rf_imp = np.mean([est.estimator.feature_importances_ for est in self.model_rf.calibrated_classifiers_], axis=0)
        except (AttributeError, Exception):
            # é™çº§æ–¹æ¡ˆ:å¦‚æœæ— æ³•ç›´æ¥è·å–,åˆ™è®¾ä¸ºç­‰æƒé‡æˆ–å°è¯•ä» base_rf è·å–
            rf_imp = np.zeros(len(feature_names))
        # æå– GBDT é‡è¦æ€§
        gbdt_imp = self.model_gbdt.get_feature_importance()
        
        # èåˆé‡è¦æ€§
        combined_imp = (rf_imp + gbdt_imp) / 2.0
        self.feature_importances = sorted(
            zip(feature_names, combined_imp), 
            key=lambda x: x[1], 
            reverse=True
        )
        
        joblib.dump((self.model_rf, self.model_mlp, self.model_gbdt, self.scaler, self.feature_importances), self.model_path)
        self.logger.info(f"ğŸš€ å…¨å±€æ··åˆæ¨¡å‹é‡è®­å®Œæˆ (çª—å£: {window})")

    def get_importance_report(self) -> List[Dict]:
        """è¿”å›æ ¼å¼åŒ–çš„ç‰¹å¾è´¡çŒ®åº¦æŠ¥å‘Š"""
        if not hasattr(self, 'feature_importances') or not self.feature_importances:
            return []
        return [{"feature": f, "importance": round(float(i), 4)} for f, i in self.feature_importances]

    def predict(self, history: List[Dict], window: int = 12) -> Dict[str, np.ndarray]:
        """å¯¹ä¸‹ä¸€æœŸç”Ÿæˆå„è·¯åŸå§‹æ¦‚ç‡çŸ©é˜µ"""
        X, _ = self.construct_features(history, window=window, mode='predict')
        X_scaled = self.scaler.transform(X)
        
        # è·å–å„è·¯åŸå§‹æ¦‚ç‡
        rf_probs = self.model_rf.predict_proba(X_scaled)[:, 1]
        mlp_probs = self.model_mlp.predict_proba(X_scaled)[:, 1]
        gbdt_probs = self.model_gbdt.predict_proba(X_scaled)[:, 1]
        
        return {
            "rf_mlp": rf_probs * 0.6 + mlp_probs * 0.4,  # åˆå¹¶ä¸º A+C
            "gbdt": gbdt_probs
        }

class PositionalEngine:
    """Stream B: ä½åºé”šç‚¹æ£®æ— (Positional Models)"""
    def __init__(self):
        self.logger = logging.getLogger("PositionalML")
        self.models = {} 
        version_tag = datetime.now().strftime('%Y%m%d')
        self.model_path = SupremeConfig.CACHE_DIR / f"pos_forest_{version_tag}.joblib"

    def train_or_load(self, history: List[Dict], data_time: float, mode: str = 'train', force: bool = False):
        """ç®¡ç† 20 ä¸ªç‹¬ç«‹æ¨¡å‹çš„æŒä¹…åŒ– (ä¼˜åŒ–: å¼•å…¥ä½åºé¢‘ç‡åˆ†å¸ƒç‰¹å¾)"""
        # ç”ŸæˆåŸºäºæ¨¡å¼çš„æ¨¡å‹è·¯å¾„
        model_name = f"pos_forest_{mode}"
        version_tag = datetime.now().strftime('%Y%m%d')
        model_path = SupremeConfig.CACHE_DIR / f"{model_name}_{version_tag}.joblib"

        if not force and model_path.exists():
            if os.path.getmtime(model_path) > data_time:
                try:
                    self.models, self.pos_freqs = joblib.load(model_path)
                    self.logger.info(f"âœ… å·²åŠ è½½å…¨éƒ¨ 20 ç»„ä½åºé”šç‚¹æ¨¡å‹ ({mode})")
                    return
                except Exception:
                    pass

        self.logger.info(f"ğŸ”„ æ­£åœ¨ä¸º 20 ä¸ªä½åºç‚¹ä½å»ºç«‹ä¸“å±æ£®æ— ({mode})...")
        
        # åˆ’åˆ†è®­ç»ƒé›†
        if mode == 'train':
            train_slice = history[:-SupremeConfig.VALIDATION_SIZE]
        else:
            train_slice = history
            
        if not train_slice:
            train_slice = history[-600:]
        
        # é¢„è®¡ç®—æ¯ä¸ªä½ç½®çš„å·ç é¢‘ç‡åˆ†å¸ƒ
        pos_freqs = {}
        for p_idx in range(20):
            all_vals = [d['ordered'][p_idx] for d in train_slice]
            counts = Counter(all_vals)
            pos_freqs[p_idx] = {n: counts.get(n, 0) / len(all_vals) for n in range(1, 81)}

        new_models = {}
        for p_idx in range(20):
            X_p, y_p = [], []
            freq_map = pos_freqs[p_idx]
            for i in range(15, len(train_slice)):
                prev_vals = [train_slice[k]['ordered'][p_idx] for k in range(i-15, i)]
                target = train_slice[i]['ordered'][p_idx]
                
                # ç‰¹å¾:æœ€è¿‘åºåˆ— + ç»Ÿè®¡é‡ + å½“å‰å·ç çš„å†å²é¢‘ç‡
                last_val = prev_vals[-1]
                feat = prev_vals + [np.mean(prev_vals), np.std(prev_vals), freq_map.get(last_val, 0)]
                X_p.append(feat)
                y_p.append(target)
            
            rf = RandomForestClassifier(**SupremeConfig.RF_POS_PARAMS)
            rf.fit(X_p, y_p)
            new_models[p_idx] = rf
            
        self.models = new_models
        self.pos_freqs = pos_freqs
        joblib.dump((self.models, self.pos_freqs), self.model_path)
        self.logger.info("ğŸš€ 20 ç»„ä½åºæ£®æ—è®­ç»ƒå®Œæˆ")

    def predict(self, history: List[Dict]) -> Dict[int, int]:
        """é¢„æµ‹ä¸‹ä¸€æœŸ 20 ä¸ªä½ç½®å¯èƒ½çš„å…·ä½“æ•°å€¼"""
        preds = {}
        recent = history[-15:]
        for p_idx, model in self.models.items():
            prev_vals = [item['ordered'][p_idx] for item in recent]
            last_val = prev_vals[-1]
            freq_map = self.pos_freqs.get(p_idx, {})
            feat = [prev_vals + [np.mean(prev_vals), np.std(prev_vals), freq_map.get(last_val, 0)]]
            preds[p_idx] = int(model.predict(feat)[0])
        return preds

class SelectEngine:
    """å®æˆ˜éªŒè¯å¼•æ“:è¯„ä¼°ç”¨æˆ·è‡ªé€‰ç»„åˆ (select2/selectX)"""
    def __init__(self):
        self.logger = logging.getLogger("SelectEngine")

    def evaluate_select_files(self, full_table: List[Dict]) -> Dict[str, List[Dict]]:
        """è¯»å–å¹¶è¯„ä¼° select ç›®å½•ä¸‹çš„æ–‡ä»¶ (ä¿®æ­£: ä½¿ç”¨ full_table å­—å…¸æé«˜æŸ¥æ‰¾æ•ˆç‡)"""
        results = {"select2": [], "selectX": []}
        global_probs = {row['num']: row['prob'] for row in full_table}
        global_scores = {row['num']: row['score'] for row in full_table}
        
        # 1. è¯„ä¼° select2 (ç»„åˆ)
        s2_file = SupremeConfig.SELECT_DIR / "select2"
        if s2_file.exists():
            try:
                with open(s2_file, 'r', encoding='utf-8') as f:
                    for line in f:
                        nums = [int(x) for x in line.replace(',', ' ').split() if x.isdigit()]
                        if len(nums) >= 2:
                            # è®¡ç®—ç»„åˆä¿¡å¿ƒåˆ† (å‡ ä½•å¹³å‡æˆ–ç®—æœ¯å¹³å‡)
                            conf = np.mean([global_probs.get(n, 0) for n in nums]) * 100
                            results["select2"].append({
                                "nums": "-".join([f"{n:02d}" for n in nums]),
                                "score": round(conf, 2)
                            })
            except Exception as e:
                self.logger.warning(f"select2 è¯»å–å¤±è´¥: {e}")

        # 2. è¯„ä¼° selectX (å•ç )
        sx_file = SupremeConfig.SELECT_DIR / "selectX"
        if sx_file.exists():
            try:
                with open(sx_file, 'r', encoding='utf-8') as f:
                    content = f.read().replace(',', ' ').split()
                    nums = sorted(list(set([int(x) for x in content if x.isdigit()])))
                    for n in nums:
                        prob = global_probs.get(n, 0)
                        results["selectX"].append({
                            "num": n,
                            "prob": round(prob, 4),
                            "score": round(prob * 100, 2)
                        })
            except Exception as e:
                self.logger.warning(f"selectX è¯»å–å¤±è´¥: {e}")
                
        return results

class ReportEngine:
    """é«˜çº§ç ”æŠ¥ç»„ä»¶åº“:å…«åˆ†åŒº, å½¢æ€åˆ†æ, å…¨é‡åˆ†æè¡¨"""
    
    @staticmethod
    def _generate_ascii_sparkline(data_list: List[float], width: int = 10) -> str:
        """
        ç”Ÿæˆå­—ç¬¦çº§è¿·ä½ å›¾ (Sparkline)
        Args:
            data_list: æ•°å€¼åˆ—è¡¨
            width: è¿‘ä¼¼å®½åº¦
        """
        if not data_list:
            return "N/A"
        
        # å½’ä¸€åŒ–
        min_val, max_val = min(data_list), max(data_list)
        if max_val == min_val:
            normalized = [0.5] * len(data_list)
        else:
            normalized = [(x - min_val) / (max_val - min_val) for x in data_list]
            
        # é™é‡‡æ ·
        if len(normalized) > width:
            step = len(normalized) / width
            resampled = [normalized[int(i * step)] for i in range(width)]
        else:
            resampled = normalized

        # æ˜ å°„å­—ç¬¦:  â–‚â–ƒâ–„â–…â–†â–‡â–ˆ
        chars = " â–‚â–ƒâ–„â–…â–†â–‡â–ˆ"
        sparkline = ""
        for val in resampled:
            index = int(val * (len(chars) - 1))
            sparkline += chars[index]
        return sparkline

    @staticmethod
    def get_basic_patterns(numbers: List[int], last_sorted: List[int] = None, history_subset: List[List[int]] = None) -> Dict:
        """è®¡ç®—åŸºç¡€å½¢æ€æŒ‡æ ‡ (æå¤§å¢å¼ºç‰ˆ:æ–°å¢ ACå€¼, è¿å·, å°¾æ•°, å†·çƒ­æ¸©)"""
        if not numbers:
            return {}

        # 1. åŸºç¡€ç»´åº¦
        odd = len([n for n in numbers if n % 2 != 0])
        big = len([n for n in numbers if n > 40])
        primes = {2, 3, 5, 7, 11, 13, 17, 19, 23, 29, 31, 37, 41, 43, 47, 53, 59, 61, 67, 71, 73, 79}
        prime_count = len([n for n in numbers if n in primes])
        
        # 2. ACå€¼ (ç®—æœ¯å¤æ‚åº¦)
        diffs = set()
        for i in range(len(numbers)):
            for j in range(i + 1, len(numbers)):
                diffs.add(abs(numbers[i] - numbers[j]))
        ac_value = len(diffs) - (len(numbers) - 1)

        # 3. è¿å·åˆ†æ (Consecutive Numbers)
        sorted_nums = sorted(numbers)
        max_consecutive = 1
        current_consecutive = 1
        consecutive_groups = 0
        for i in range(len(sorted_nums) - 1):
            if sorted_nums[i+1] - sorted_nums[i] == 1:
                current_consecutive += 1
            else:
                if current_consecutive > 1:
                    consecutive_groups += 1
                    max_consecutive = max(max_consecutive, current_consecutive)
                current_consecutive = 1
        if current_consecutive > 1:
            consecutive_groups += 1
            max_consecutive = max(max_consecutive, current_consecutive)

        # 4. å°¾æ•°åˆ†å¸ƒ (Tail distribution)
        tails = [n % 10 for n in numbers]
        tail_counts = Counter(tails)
        tail_str = ":".join([str(tail_counts.get(i, 0)) for i in range(10)])
        
        # 5. é‡å·ä¸é‚»å· (Cross-period)
        repeat_count = 0
        neighbor_count = 0
        if last_sorted:
            last_set = set(last_sorted)
            repeat_count = len(set(numbers) & last_set)
            # é‚»å·:æœ¬æœŸå·ç åœ¨ä¸Šä¸€æœŸå·ç çš„ Â±1 èŒƒå›´å†…
            neighbor_set = set()
            for n in last_sorted:
                neighbor_set.add(n-1)
                neighbor_set.add(n+1)
            neighbor_count = len(set(numbers) & neighbor_set)

        # 6. å†·çƒ­æ¸©åˆ†æ (åŸºäº history_subset)
        cold_hot_warm = {"hot": 0, "warm": 0, "cold": 0}
        if history_subset:
            # å…¼å®¹å¤„ç†:history_subset å¯èƒ½æ˜¯ Dict åˆ—è¡¨æˆ– List åˆ—è¡¨
            clean_history = [row['sorted'] if isinstance(row, dict) else row for row in history_subset]
            flat_history = [n for row in clean_history for n in row]
            counts = Counter(flat_history)
            threshold_hot = len(clean_history) * 20 / 80 * 1.2  # é«˜äºå¹³å‡ 20%
            threshold_cold = len(clean_history) * 20 / 80 * 0.8  # ä½äºå¹³å‡ 20%
            for n in numbers:
                freq = counts.get(n, 0)
                if freq >= threshold_hot:
                    cold_hot_warm["hot"] += 1
                elif freq <= threshold_cold:
                    cold_hot_warm["cold"] += 1
                else:
                    cold_hot_warm["warm"] += 1

        # 7. Hurst æŒ‡æ•° (åŸºäº history_subset)
        hurst_val = 0.5
        if history_subset:
            clean_history = [row['sorted'] if isinstance(row, dict) else row for row in history_subset]
            sums = np.array([sum(row) for row in clean_history])
            hurst_val = PhysicsEngine.calculate_hurst(sums)

        # 8. è±¡é™åˆ†å¸ƒ (Quadrant distribution: 1-16, 17-32, 33-48, 49-64, 65-80)
        quadrants = [0, 0, 0, 0, 0]
        for n in numbers:
            if 1 <= n <= 16:
                quadrants[0] += 1
            elif 17 <= n <= 32:
                quadrants[1] += 1
            elif 33 <= n <= 48:
                quadrants[2] += 1
            elif 49 <= n <= 64:
                quadrants[3] += 1
            elif 65 <= n <= 80:
                quadrants[4] += 1
        quadrant_str = ":".join(map(str, quadrants))

        return {
            "numbers": numbers,
            "odd_even": f"{odd}:{20-odd}",
            "big_small": f"{big}:{20-big}",
            "prime_composite": f"{prime_count}:{20-prime_count}",
            "ac": ac_value,
            "max_consecutive": max_consecutive,
            "consecutive_groups": consecutive_groups,
            "tails": tail_str,
            "sum": sum(numbers),
            "span": max(numbers) - min(numbers) if numbers else 0,
            "repeat": repeat_count,
            "neighbor": neighbor_count,
            "chw": f"{cold_hot_warm['hot']}:{cold_hot_warm['warm']}:{cold_hot_warm['cold']}",
            "hurst": round(hurst_val, 4),
            "quadrants": quadrant_str
        }

    @staticmethod
    def calculate_quadrants(full_table: List[Dict]) -> List[Dict]:
        """è®¡ç®—äº”è±¡é™èƒ½é‡åˆ†å¸ƒ (1-16, 17-32, 33-48, 49-64, 65-80)"""
        probs = {row['num']: row['prob'] for row in full_table}
        return ReportEngine.get_quadrant_analysis(probs)

    @staticmethod
    def calculate_kelly_sizing(resonance_picks: List[Dict]) -> Dict:
        """åŸºäºå‡¯åˆ©å…¬å¼ (Kelly Criterion) æä¾›ä»“ä½å»ºè®® (ä» README æ¢å¤)"""
        # ç®€åŒ–ç‰ˆå‡¯åˆ©: f* = (p*b - q) / b
        # p: èƒœç‡ (prob), b: èµ”ç‡ (å‡è®¾ä¸ºå¸¸æ•° 3.5), q: è´¥ç‡ (1-p)
        # f* = (p * (b+1) - 1) / b
        b = 3.5 
        advice = []
        for r in resonance_picks[:5]:  # ä»…å¯¹å‰ 5 ä¸ªå…±æŒ¯å·è¿›è¡Œå»ºè®®
            p = r['prob']
            f_star = (p * (b + 1) - 1) / b
            if f_star > 0:
                # é™åˆ¶æœ€å¤§ä»“ä½ä¸º 15% é¿å…è¿‡æ¿€
                suggested = min(f_star, 0.15)
                advice.append({
                    "num": r['num'],
                    "prob": p,
                    "sizing": f"{suggested*100:.1f}%",
                    "level": "ğŸš€ æ¿€è¿›" if suggested > 0.1 else "âš–ï¸ ç¨³å¥"
                })
        return {"advice": advice, "summary": "å»ºè®®é‡‡ç”¨åˆ†ä»“åˆ†æ‰¹å…¥åœº,ä¸¥æ§æœ€å¤§å›æ’¤"}

    @staticmethod
    def get_quadrant_analysis(probs: Dict[int, float]) -> List[Dict]:
        """æ‰§è¡Œäº”è±¡é™èƒ½é‡å¯†åº¦åˆ†æ (1-16, 17-32, 33-48, 49-64, 65-80)"""
        quads = []
        for i in range(5):
            start, end = i*16 + 1, (i+1)*16
            quad_nums = [n for n in range(start, end+1)]
            avg_prob = np.mean([probs.get(n, 0) for n in quad_nums])
            hot_nums = sorted(quad_nums, key=lambda n: probs.get(n, 0), reverse=True)[:4]
            
            rating = "ğŸ”¥" * int(avg_prob * 30)  # è±¡é™çƒ­åº¦
            quads.append({
                "range": f"{start:02d} -{end:02d}",
                "avg_prob": round(avg_prob, 4),
                "hot_nums": hot_nums,
                "rating": rating if rating else "ğŸ’¤"
            })
        return quads

    @staticmethod
    def get_resonance_picks(global_probs: Dict[int, float], pos_preds: Dict[int, int]) -> List[Dict]:
        """å¯»æ‰¾å…¨å±€é«˜æ¦‚ç‡ä¸ç‚¹ä½é¢„æµ‹çš„å…±æŒ¯å·ç """
        # å–å…¨å±€ Top 25
        top_global = sorted(global_probs.keys(), key=lambda n: global_probs[n], reverse=True)[:25]
        # å–ç‚¹ä½é¢„æµ‹å»é‡
        pos_nums = set(pos_preds.values())
        
        resonance = []
        for n in sorted(list(pos_nums)):
            if n in top_global:
                prob = global_probs[n]
                # æ¨èç­‰çº§
                stars = "â­â­â­â­â­" if prob > 0.28 else "â­â­â­â­"
                resonance.append({"num": n, "prob": round(prob, 4), "level": stars})
        
        return sorted(resonance, key=lambda x: x['prob'], reverse=True)

    @staticmethod
    def get_vertical_analysis(pos_preds: Dict[int, int], global_probs: Dict[int, float], history: List[Dict]) -> List[Dict]:
        """20ç‚¹ä½å‚ç›´åˆ†å¸ƒäº¤å‰éªŒè¯ (å¢å¼ºç‰ˆ:ä¸€ä½ç½®ä¸€è¡Œå¤šç»´åº¦æŒ‡æ ‡)"""
        top_global = sorted(global_probs.keys(), key=lambda n: global_probs[n], reverse=True)[:20]
        
        # è·å–é—æ¼ä¿¡æ¯
        last_idx = len(history)
        appearances = defaultdict(list)
        for idx, item in enumerate(history):
            for n in item['sorted']:
                appearances[n].append(idx)
        
        vertical = []
        for i in range(20):
            num = pos_preds.get(i)
            prob = global_probs.get(num, 0)
            
            # è®¡ç®—è¯¥å·ç çš„é—æ¼
            idx_pos = bisect.bisect_left(appearances[num], last_idx)
            gap = last_idx - appearances[num][idx_pos-1] - 1 if idx_pos > 0 else last_idx
            
            is_match = "âœ… **åŒæµåˆä¸€**" if num in top_global else "âš ï¸ ä»…ç‚¹ä½çœ‹å¥½"
            
            # ä¿¡å¿ƒåˆ†è¯„çº§
            score = prob * 100
            rating = "â­â­â­â­â­" if score > 28 else "â­â­â­â­" if score > 26 else "â­â­â­"
            
            vertical.append({
                "pos": i + 1,
                "num": num,
                "prob": round(prob, 4),
                "gap": gap,
                "score": round(score, 2),
                "rating": rating,
                "check": is_match
            })
        return vertical

    @staticmethod
    def get_zone_analysis(probs: Dict[int, float]) -> List[Dict]:
        """æ‰§è¡Œå…«åˆ†åŒºèƒ½é‡å¯†åº¦åˆ†æ"""
        zones = []
        for i in range(8):
            start, end = i*10 + 1, (i+1)*10
            zone_nums = [n for n in range(start, end+1)]
            avg_prob = np.mean([probs.get(n, 0) for n in zone_nums])
            hot_nums = sorted(zone_nums, key=lambda n: probs.get(n, 0), reverse=True)[:5]
            
            rating = "â­" * int(avg_prob * 40)  # åŠ¨æ€è¯„çº§
            zones.append({
                "range": f"{start:02d} -{end:02d}",
                "avg_prob": round(avg_prob, 4),
                "hot_nums": hot_nums,
                "rating": rating if rating else "-"
            })
        return zones

    @staticmethod
    def get_full_table(probs: Dict[int, float], history: List[Dict]) -> List[Dict]:
        """ç”Ÿæˆ 80 å·ç å…¨é‡åˆ†ææ•°æ®"""
        last_sorted = history[-1]['sorted'] if history else []
        appearances = defaultdict(list)
        for idx, item in enumerate(history):
            for n in item['sorted']:
                appearances[n].append(idx)
        
        last_idx = len(history)
        table = []
        for n in range(1, 81):
            prob = probs.get(n, 0)
            idx_pos = bisect.bisect_left(appearances[n], last_idx)
            gap = last_idx - appearances[n][idx_pos-1] - 1 if idx_pos > 0 else last_idx
            
            # è¶‹åŠ¿é€»è¾‘
            if prob > 0.28:
                trend = "ğŸ”¥ Strong"
            elif prob > 0.26 and gap > 10:
                trend = "ğŸ“ˆ Rebound"
            elif prob > 0.25:
                trend = "âš–ï¸ Stable"
            elif prob < 0.20:
                trend = "â„ï¸ Weak"
            else:
                trend = "â¡ï¸"
                
            table.append({
                "num": n,
                "prob": round(prob, 4),
                "gap": gap,
                "score": round(prob * 100, 2),
                "trend": trend
            })
        return table

    @staticmethod
    def export_omission_stats(full_table: List[Dict]):
        """å°†é—æ¼ç»Ÿè®¡å›å†™åˆ°æœ€æ–°çš„å†å²ç›®å½•"""
        if not SupremeConfig.HISTORY_BASE_DIR.exists():
            return
            
        try:
            # è·å–æœ€æ–°çš„æ—¥æœŸç›®å½•
            history_dirs = [d for d in SupremeConfig.HISTORY_BASE_DIR.iterdir() if d.is_dir()]
            if not history_dirs:
                return
            
            latest_dir = max(history_dirs, key=lambda x: x.name)
            file_path = latest_dir / "omission_stats.txt"
            
            # æŒ‰é—æ¼å€¼åˆ†ç»„,æ¨¡æ‹ŸåŸæœ‰ç´§å‡‘æ ¼å¼
            gap_groups = defaultdict(list)
            for row in full_table:
                gap_groups[row['gap']].append(row['num'])
            
            with open(file_path, 'w', encoding='utf-8') as f:
                f.write(f"--- é—æ¼å€¼åˆ†å¸ƒç»Ÿè®¡ ---\n")
                f.write(f"æ›´æ–°æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n\n")
                # æŒ‰é—æ¼å€¼ä»å¤§åˆ°å°æ’åˆ—
                for gap in sorted(gap_groups.keys(), reverse=True):
                    nums = gap_groups[gap]
                    num_str = "".join([f"{n:02d}" for n in sorted(nums)])
                    f.write(f"é—æ¼ {gap:02d} æœŸ: {num_str}\n")
            
            logging.info(f"âœ… é—æ¼ç»Ÿè®¡å·²åŒæ­¥è‡³ {latest_dir.name}")
        except Exception as e:
            logging.warning(f"âš ï¸ é—æ¼ç»Ÿè®¡å›å†™å¤±è´¥: {e}")

class KernelEngine:
    """æ ¸å¿ƒç‚¹ä½èåˆä¸è£‚å˜å¼•æ“ (å¢å¼ºç‰ˆ: äº”æµåˆä¸€ + Hurst åŠ¨æ€èµ‹æƒ)"""
    @staticmethod
    def generate_smart_pool(
        global_probs_dict: Dict[str, np.ndarray], 
        pos_preds: Dict[int, int], 
        history: List[Dict], 
        tcn_probs: Dict[int, float] = None,
        arima_probs: Dict[int, float] = None,
        loaded_core_points: List[int] = None
    ) -> Dict:
        """å¤šç»´èåˆç”Ÿæˆæ™ºèƒ½æ‰©å±•æ±  (Stream A+B+C+D+E+GBDT)"""
        w = SupremeConfig.FUSION_WEIGHTS
        final_probs = {}
        
        # 1. æå–å„è·¯æ¦‚ç‡
        rf_mlp_probs = global_probs_dict.get("rf_mlp", np.zeros(80))
        gbdt_probs = global_probs_dict.get("gbdt", np.zeros(80))
        
        # 2. è®¡ç®—ç›˜é¢æ•´ä½“ Hurst ä»¥è°ƒæ•´å…¨å±€æƒé‡
        sums = np.array([sum(d['sorted']) for d in history[-50:]]) if history else np.array([0])
        overall_hurst = PhysicsEngine.calculate_hurst(sums)
        
        # 3. æ‰§è¡Œäº”æµèåˆåŠ æƒ
        for n in range(1, 81):
            idx = n - 1
            p = rf_mlp_probs[idx] * w['rf_mlp'] + gbdt_probs[idx] * w['gbdt']
            
            if tcn_probs:
                p += tcn_probs.get(n, 0) * w['tcn']
            if arima_probs:
                p += arima_probs.get(n, 0) * w['arima']
            
            # 4. Hurst åŠ¨æ€å¢å¼º (å¦‚æœè¶‹åŠ¿æå¼º,å¯¹çƒ­å·åŠ æƒ)
            if overall_hurst > 0.6 and p > 0.25:
                p *= (1.0 + (overall_hurst - 0.6))
            
            final_probs[n] = float(p)

        # 1. æ ¸å¿ƒ 20 ç‚¹ä½ (ä½åºæ¨¡å‹é¢„æµ‹å€¼ + å¤–éƒ¨åŠ è½½ç‚¹ä½)
        pos_core = set(pos_preds.values())
        if loaded_core_points:
            # èåˆå¤–éƒ¨ç‚¹ä½,è‹¥è¶…è¿‡ 20 ä¸ªåˆ™æ ¹æ®æ¦‚ç‡ç­›é€‰
            combined_core = pos_core | set(loaded_core_points)
            if len(combined_core) > 20:
                core_20 = sorted(list(combined_core), key=lambda n: final_probs.get(n, 0), reverse=True)[:20]
            else:
                core_20 = sorted(list(combined_core))
        else:
            core_20 = sorted(list(pos_core))
        
        # 2. å…¨å±€é«˜æ¦‚å·ç  (Top 40)
        global_top_40 = sorted(final_probs.keys(), key=lambda n: final_probs[n], reverse=True)[:40]
        
        # 3. æ™ºèƒ½æ‰©å±•æ±  (æ ¸å¿ƒ + å…¨å±€é«˜æ¦‚å¹¶é›†)
        smart_pool = sorted(list(set(core_20) | set(global_top_40)))
        
        # 4. è®¡ç®—å…±æŒ¯
        resonance_picks = ReportEngine.get_resonance_picks(final_probs, pos_preds)
        vertical_analysis = ReportEngine.get_vertical_analysis(pos_preds, final_probs, history)
        
        # 5. æŒ–æ˜å…³è”è§„åˆ™ä¸è·Ÿéšå¼ºåº¦ (New)
        assoc_rules = AssociationEngine.mine_rules(history)
        follower_rules = FollowerEngine.analyze_followers(history)
        
        return {
            "core_20": core_20,
            "smart_pool": smart_pool,
            "resonance_count": len(resonance_picks),
            "resonance_picks": resonance_picks,
            "vertical_analysis": vertical_analysis,
            "assoc_rules": assoc_rules,
            "follower_rules": follower_rules,
            "overall_hurst": round(overall_hurst, 4),
            "regime": MarketEngine.analyze_regime(history),
            "last_patterns": ReportEngine.get_basic_patterns(
                history[-1]['sorted'], 
                history[-2]['sorted'] if len(history) > 1 else None,
                history[-50:]
            ),
            "zones": ReportEngine.get_zone_analysis(final_probs),
            "full_table": ReportEngine.get_full_table(final_probs, history)
        }

# ==========================================
# 4. è‡ªåŠ¨åŒ–éªŒè¯å¼•æ“ (Backtest Engine)
# ==========================================

class AutoValidationEngine:
    """è‡ªåŠ¨åŒ–å›æµ‹ä¸å¯¹è´¦å¼•æ“ (é«˜æ•ˆæ¨ç†ç‰ˆ)"""
    def __init__(self, data_engine: DataEngine, global_engine: MLEngine, pos_engine: PositionalEngine):
        self.data_engine = data_engine
        self.global_engine = global_engine
        self.pos_engine = pos_engine
        self.results = []

    def run_backtest(self, periods: int = 30, params: Dict = None):
        """æ‰§è¡Œé«˜æ•ˆæ»šåŠ¨çª—å£å›æµ‹ (æ”¯æŒå‚æ•°æ³¨å…¥)"""
        logging.info(f"ğŸ”„ å¯åŠ¨æ·±åº¦å›æµ‹: ç›‘æµ‹æœ€è¿‘ {periods} æœŸ...")
        history = self.data_engine.history
        total = len(history)
        start_idx = total - periods
        
        # å¦‚æœæä¾›äº†å‚æ•°,åˆ™æ³¨å…¥ (AutoTuner ä½¿ç”¨)
        if params:
            if 'rf_mlp' in params:
                SupremeConfig.FUSION_WEIGHTS['rf_mlp'] = params['rf_mlp']
            if 'gbdt' in params:
                SupremeConfig.FUSION_WEIGHTS['gbdt'] = params['gbdt']
            if 'tcn' in params:
                SupremeConfig.FUSION_WEIGHTS['tcn'] = params['tcn']
            if 'arima' in params:
                SupremeConfig.FUSION_WEIGHTS['arima'] = params['arima']

        
        # [ä¼˜åŒ–] æ·»åŠ  tqdm è¿›åº¦æ¡
        results = []
        loop_iterator = tqdm(range(start_idx, total), desc="running backtest", unit="period")
        for i in loop_iterator:
            known_history = history[:i]
            target_real = set(history[i]['sorted'])
            
            # å„è·¯é¢„æµ‹ (å›æµ‹æ¨¡å¼ä¸‹ä¸é‡è®­)
            probs_dict = self.global_engine.predict(known_history)
            bt_pos_preds = self.pos_engine.predict(known_history)
            
            # ç®€åŒ–å›æµ‹:ä¸è¿è¡Œè€—æ—¶è¾ƒé•¿çš„ TCN/ARIMA,ä»…éªŒè¯ A+B+C+GBDT
            pool_info = KernelEngine.generate_smart_pool(probs_dict, bt_pos_preds, known_history)
            smart_pool = pool_info['smart_pool']
            core_20 = pool_info['core_20']
            
            hits = len(target_real.intersection(smart_pool))
            core_hits = len(target_real.intersection(core_20))
            
            results.append({
                'period': history[i]['period'],
                'pool_size': len(smart_pool),
                'hits': hits,
                'core_hits': core_hits,
                'pnl': hits - (len(smart_pool) * 0.1)
            })
        
        self.results = results
        return np.mean([r['hits'] for r in results]) if results else 0

    def run_backtest_full(self, periods: int = 15, params: Dict = None, tcn_probs_stream: Dict = None, arima_probs_stream: Dict = None):
        """æ‰§è¡Œå…¨æµé›†æˆæ»šåŠ¨å›æµ‹ (AutoTuner ä¸“ç”¨,æœ€å¤§åŒ–ç²¾åº¦)"""
        history = self.data_engine.history
        total = len(history)
        start_idx = total - periods
        
        results = []
        # [ä¼˜åŒ–] æ·»åŠ  tqdm è¿›åº¦æ¡ (nested=True)
        loop_iterator = tqdm(range(start_idx, total), desc="tuning backtest", unit="period", leave=False)
        for i in loop_iterator:
            known_history = history[:i]
            target_real = set(history[i]['sorted'])
            
            # 1. è·å–å„è·¯åŸºç¡€é¢„æµ‹
            probs_dict = self.global_engine.predict(known_history)
            bt_pos_preds = self.pos_engine.predict(known_history)
            
            # 2. æ³¨å…¥ TCN å’Œ ARIMA é¢„æµ‹ (å¦‚æœæä¾›)
            tcn_p = tcn_probs_stream.get(i) if tcn_probs_stream else None
            arima_p = arima_probs_stream.get(i) if arima_probs_stream else None
            
            # 3. æ ¸å¿ƒèåˆ
            pool_info = KernelEngine.generate_smart_pool(
                probs_dict, bt_pos_preds, known_history,
                tcn_probs=tcn_p,
                arima_probs=arima_p
            )
            
            hits = len(target_real.intersection(pool_info['smart_pool']))
            results.append(hits)
            
        return np.mean(results) if results else 0

    def generate_validation_report(self) -> str:
        """ç”Ÿæˆ Markdown æ ¼å¼çš„è¯¦ç»†éªŒè¯å¯¹è´¦å•"""
        if not self.results:
            return "æ— å›æµ‹æ•°æ®"
        
        avg_hits = np.mean([r['hits'] for r in self.results])
        avg_core = np.mean([r['core_hits'] for r in self.results])
        total_pnl = sum([r['pnl'] for r in self.results])
        
        img_filename = "backtest_curve_unified.png"
        img_path = SupremeConfig.REPORT_DIR / img_filename
        
        plt.figure(figsize=(10, 5))
        cum_hits = np.cumsum([r['hits'] for r in self.results])
        plt.plot(cum_hits, label='ç´¯è®¡å‘½ä¸­æ•°', color='#1f77b4', marker='o')
        plt.title(f"ç³»ç»Ÿæœ€è¿‘ {len(self.results)} æœŸå‘½ä¸­éªŒè¯æ›²çº¿")
        plt.xlabel("æµ‹è¯•æœŸæ•°")
        plt.ylabel("ç´¯è®¡å‘½ä¸­")
        plt.grid(True, alpha=0.3)
        plt.legend()
        plt.tight_layout()
        plt.savefig(img_path)
        plt.close()
        
        avg_hits_str = f"{avg_hits:.2f}"
        avg_core_str = f"{avg_core:.2f}"
        total_pnl_str = f"{total_pnl:.2f}"
        
        report = f"""
### ğŸ§ª ç³»ç»Ÿå›æµ‹éªŒè¯æŠ¥å‘Š (Supreme Validation)
---
**éªŒè¯æœŸæ•°**: {len(self.results)} æœŸ
**å¹³å‡å‘½ä¸­ç‡ (Smart Pool)**: {avg_hits_str} ä¸ª/æœŸ
**å¹³å‡æ ¸å¿ƒå‘½ä¸­ (Core 20)**: {avg_core_str} ä¸ª/æœŸ
**ç´¯è®¡è™šæ‹Ÿå¢ç›ŠæŒ‡æ ‡**: {total_pnl_str}

#### ğŸ“ˆ å‘½ä¸­å¢é•¿æ›²çº¿
![Backtest Curve]({img_filename})

| æœŸå· | å›æµ‹å‘½ä¸­(Pool) | æ ¸å¿ƒå‘½ä¸­(Core) | æ± å¤§å° | æ”¶ç›ŠçŠ¶æ€ |
| :--- | :--- | :--- | :--- | :--- |
"""
        for r in self.results[-10:]:
            status = "ğŸ’¹" if r['hits'] >= 6 else "ğŸ“Š"
            report += f"| {r['period']} | {r['hits']} | {r['core_hits']} | {r['pool_size']} | {status} |\n"
            
        return report

# ==========================================
# 5. ä¸»ç¨‹åºç®¡å®¶ (Supreme Manager)
# ==========================================

class SupremeManager:
    """ä¸€ä½“åŒ–è¿è¡Œæ€»æ§ (Supreme Unified Edition)"""
    def __init__(self):
        SupremeConfig.init_environment()
        self.logger = logging.getLogger("SupremeManager")
        self.data_engine = DataEngine()
        self.global_ml = MLEngine()
        self.pos_ml = PositionalEngine()
        self.select_engine = SelectEngine()
        self.tcn_engine = TCNEngine()
        self.arima_engine = ARIMAEngine()
        self.follower_engine = FollowerEngine()
        
    def run_production_pipeline(self, run_backtest: bool = True, persist_models: bool = True, incremental: bool = False, auto_tune: bool = True):
        """æ‰§è¡Œæ­£å¼ç”Ÿäº§é¢„æµ‹é€»è¾‘ (è‡ªé€‚åº”çª—å£ + è‡ªåŠ¨è°ƒä¼˜ + å…¨å¼•æ“ç‰ˆ)"""
        data_time = self.data_engine.get_last_timestamp()
        history = self.data_engine.history
        
        # 0. å¸‚åœºæ„ŸçŸ¥è·å–æ¨èçª—å£
        regime_info = MarketEngine.analyze_regime(history)
        rec_window = regime_info['recommended_window']
        self.logger.info(f"ğŸ” å¸‚åœºæ„ŸçŸ¥: {regime_info['status']}, æ¨èçª—å£: {rec_window}")

        # [æ–°å¢] æ¬¡æ—¥éªŒè¯:æ£€æŸ¥æ˜¨æ—¥é¢„æµ‹å‘½ä¸­æƒ…å†µ
        self.verify_yesterday_prediction()

        # 1. æ¨¡å‹å‡†å¤‡ (ä¼ é€’è‡ªé€‚åº”çª—å£)
        # ç”Ÿäº§æ¨¡å¼ä¸‹,ä½¿ç”¨å…¨é‡æ•°æ®è¿›è¡Œæœ€ç»ˆé¢„æµ‹è®­ç»ƒ (mode='production')
        self.global_ml.train_or_load(history, data_time, window=rec_window, mode='production')
        self.pos_ml.train_or_load(history, data_time, mode='production')
        self.tcn_engine.train_or_load(history, data_time, mode='production')
        
        # 2. è‡ªåŠ¨è°ƒä¼˜ (AutoTuner)
        # AutoTuner å†…éƒ¨ä¼šä½¿ç”¨ mode='train' (History - 300) å’Œ mode='validate' (Latest 300)
        if auto_tune and SupremeConfig.AUTO_TUNE_ENABLED:
            tuner = AutoTuner(self)
            tuner.tune()

        # 3. ç”Ÿæˆæœ€æ–°é¢„æµ‹ (äº”æµåˆä¸€)
        probs_dict = self.global_ml.predict(history, window=SupremeConfig.WINDOW_SIZE)
        pos_preds = self.pos_ml.predict(history)
        tcn_probs = self.tcn_engine.predict(history)
        arima_probs = self.arima_engine.predict(history)
        
        # 4. æŒ–æ˜å…³è”è§„åˆ™ä¸è·Ÿéšå¼ºåº¦
        assoc_rules = self.global_ml.model_gbdt.mine_association_rules(history)
        follower_rules = self.follower_engine.analyze_followers(history)
        # [æ–°å¢] å›å†™è·Ÿéšä¸é¢‘æ¬¡ç»Ÿè®¡åˆ° history ç›®å½•
        self.follower_engine.export_follower_stats(history, follower_rules)
        
        # 5. æ ¸å¿ƒèåˆ
        final_result = KernelEngine.generate_smart_pool(
            probs_dict, pos_preds, history, 
            tcn_probs=tcn_probs, 
            arima_probs=arima_probs,
            loaded_core_points=self.data_engine.core_points
        )
        
        # æ³¨å…¥å…³è”è§„åˆ™ä¸è·Ÿéšç»“æœè‡³ final_result ä»¥ä¾›æŠ¥å‘Šç”Ÿæˆ
        final_result['assoc_rules'] = assoc_rules
        final_result['follower_rules'] = follower_rules
        
        # 6. å®æˆ˜éªŒè¯
        select_results = self.select_engine.evaluate_select_files(final_result['full_table'])
        
        # 7. è®¡ç®—è±¡é™åˆ†æä¸ä»“ä½å»ºè®® (ä» README æ¢å¤çš„é«˜ä»·å€¼åŠŸèƒ½)
        final_result['quadrants'] = ReportEngine.calculate_quadrants(final_result['full_table'])
        final_result['kelly_advice'] = ReportEngine.calculate_kelly_sizing(final_result['resonance_picks'])
        
        # [æ–°å¢] å›å†™é—æ¼ç»Ÿè®¡åˆ° history ç›®å½•
        ReportEngine.export_omission_stats(final_result['full_table'])
        
        # 8. è‡ªåŠ¨åŒ–å›æµ‹ (ä½¿ç”¨æœ€ç»ˆè°ƒä¼˜åçš„å‚æ•°)
        val_md = ""
        if run_backtest:
            validator = AutoValidationEngine(self.data_engine, self.global_ml, self.pos_ml)
            validator.run_backtest(periods=SupremeConfig.VALIDATION_SIZE)
            val_md = validator.generate_validation_report()
        
        # 8. ç”Ÿæˆæœ€ç»ˆæŠ¥å‘Š
        self._generate_final_report(final_result, val_md, select_results)
        
        # 9. [æ–°å¢] å­˜æ¡£é¢„æµ‹ç»“æœä¾›æ¬¡æ—¥æ¯”å¯¹
        self.archive_prediction(final_result)
        
        # 10. æŒä¹…åŒ–æ§åˆ¶
        if not persist_models:
            try:
                if self.global_ml.model_path.exists():
                    os.remove(self.global_ml.model_path)
                if self.pos_ml.model_path.exists():
                    os.remove(self.pos_ml.model_path)
                self.logger.info("å·²æ¸…ç†ä¸´æ—¶æ¨¡å‹æ–‡ä»¶")
            except Exception as e:
                self.logger.warning(f"æ¨¡å‹æ¸…ç†æç¤º: {e}")

    def verify_yesterday_prediction(self):
        """[é¦–å¸­é€»è¾‘] æ¬¡æ—¥è‡ªåŠ¨éªŒè¯:è¯»å–æ˜¨æ—¥é¢„æµ‹ç»“æœå¹¶æ¯”å¯¹æœ€æ–°æ•°æ®å‘½ä¸­ç‡"""
        archive_path = SupremeConfig.BASE_DIR / "data" / "last_prediction.json"
        if not archive_path.exists():
            self.logger.info("â„¹ï¸ æœªå‘ç°æ˜¨æ—¥é¢„æµ‹å­˜æ¡£,è·³è¿‡æ¬¡æ—¥éªŒè¯.")
            return

        try:
            with open(archive_path, 'r', encoding='utf-8') as f:
                last_pred = json.load(f)
            
            last_period = last_pred.get('predict_period')
            # åœ¨æœ€æ–°å†å²ä¸­å¯»æ‰¾è¯¥æœŸå·çš„çœŸå®å¼€å¥–
            history = self.data_engine.history
            actual_draw = next((d for d in history if d['period'] == last_period), None)
            
            if not actual_draw:
                self.logger.info(f"â³ æ˜¨æ—¥é¢„æµ‹æœŸå· {last_period} å°šæœªå¼€å¥–,ç­‰å¾…æ–°æ•°æ®æ‹‰å–.")
                return
            
            # æ‰§è¡Œæ¯”å¯¹
            real_nums = set(actual_draw['sorted'])
            core_20 = set(last_pred.get('core_20', []))
            smart_pool = set(last_pred.get('smart_pool', []))
            
            core_hits = len(real_nums.intersection(core_20))
            pool_hits = len(real_nums.intersection(smart_pool))
            
            self.logger.info("=" * 50)
            self.logger.info(f"âœ… æ˜¨æ—¥é¢„æµ‹éªŒè¯æˆåŠŸ (æœŸå·: {last_period})")
            self.logger.info(f"   - æ ¸å¿ƒ 20 å‘½ä¸­: {core_hits} / 20")
            self.logger.info(f"   - æ™ºèƒ½å¤§åº•å‘½ä¸­: {pool_hits} / {len(smart_pool)}")
            
            # [ç­–ç•¥è°ƒæ•´é€»è¾‘]:å¦‚æœå‘½ä¸­ç‡è¿‡ä½,å¼ºåˆ¶è§¦å‘æœ¬è½® AutoTune
            if core_hits < 3 or pool_hits < 8:
                self.logger.warning("âš ï¸ æ˜¨æ—¥å‘½ä¸­ç‡åä½,ç³»ç»Ÿå°†è‡ªåŠ¨è§¦å‘æœ¬è½®æ·±åº¦è°ƒä¼˜ (AutoTune Force ON)")
                SupremeConfig.AUTO_TUNE_ENABLED = True
                SupremeConfig.AUTO_TUNE_TRIALS = max(SupremeConfig.AUTO_TUNE_TRIALS, 40)  # å¢åŠ æœç´¢æ·±åº¦
            self.logger.info("=" * 50)
            
            # éªŒè¯å®Œæˆåé‡å‘½åæˆ–æ¸…ç†,é¿å…é‡å¤éªŒè¯
            archive_path.rename(archive_path.with_name(f"verified_{last_period}.json"))
            
        except Exception as e:
            self.logger.error(f"âŒ æ˜¨æ—¥é¢„æµ‹éªŒè¯å¤±è´¥: {e}")

    def archive_prediction(self, result: Dict):
        """[æ•°æ®ç•™å­˜] å°†å½“å‰é¢„æµ‹ç»“æœç»“æ„åŒ–å­˜æ¡£,ä¾›æ¬¡æ—¥è‡ªåŠ¨åŒ–æ¯”å¯¹éªŒè¯"""
        archive_path = SupremeConfig.BASE_DIR / "data" / "last_prediction.json"
        try:
            # ç¡®å®šé¢„æµ‹çš„ä¸‹ä¸€æœŸæœŸå· (å‡è®¾å†å²æœ€åä¸€æœŸ + 1)
            last_hist_period = self.data_engine.history[-1]['period']
            try:
                # å°è¯•è§£ææœŸå·,å¤„ç†å¦‚ 20260114 è¿™ç§æ ¼å¼
                next_period = str(int(last_hist_period) + 1)
            except Exception:
                next_period = "UNKNOWN_NEXT"
                
            archive_data = {
                "predict_date": datetime.now().strftime('%Y-%m-%d'),
                "predict_period": next_period,
                "core_20": [int(n) for n in result['core_20']],
                "smart_pool": [int(n) for n in result['smart_pool']],
                "resonance_picks": [{"num": int(r['num']), "prob": r['prob']} for r in result['resonance_picks'][:10]]
            }
            
            with open(archive_path, 'w', encoding='utf-8') as f:
                json.dump(archive_data, f, ensure_ascii=False, indent=4)
            self.logger.info(f"ğŸ“‚ é¢„æµ‹ç»“æœå·²å­˜æ¡£è‡³ {archive_path.name}, å¾…æ¬¡æ—¥éªŒè¯.")
        except Exception as e:
            self.logger.error(f"âŒ é¢„æµ‹å­˜æ¡£å¤±è´¥: {e}")

    def _generate_final_report(self, result: Dict, validation_md: str, select_results: Dict = None):
        """ç”Ÿæˆå…¨ç»´åº¦ä¸€ä½“åŒ–é‡åŒ–ç ”æŠ¥ (è¶…è¶Š 160014 ç‰ˆæœ¬,æåº¦è¯¦å°½ç‰ˆ)"""
        report_path = SupremeConfig.REPORT_DIR / f"Supreme_Quant_Analysis_{datetime.now().strftime('%Y%m%d_%H%M%S')}.md"
        
        core_20_str = " ".join([f"{n:02d}" for n in result['core_20']])
        smart_pool = result['smart_pool']
        pool_str = " ".join([f"{n:02d}" for n in smart_pool])
        
        regime = result['regime']
        patterns = result['last_patterns']
        
        # 1. æŠ¥å‘Šå¯¼èˆªè¡¨
        nav_md = """| æ¨¡å— | æ ¸å¿ƒå†…å®¹ | å…³é”®æŒ‡æ ‡ |
|:---:|:---|:---|
| **[Â§1 åŒæµæ ¸å¿ƒ](#-1-åŒæµæ ¸å¿ƒ-dual-stream-intelligence)** | äº”æµèåˆ (RF/MLP/TCN/ARIMA/GBT) | **å…±æŒ¯å·ç  / æ¨èç­‰çº§** |
| **[Â§2 å¸‚åœºæ„ŸçŸ¥](#-3-å¸‚åœºç¯å¢ƒæ„ŸçŸ¥-market-regime)** | è¶‹åŠ¿æ–œç‡ / æ³¢åŠ¨ç‡ / ç†µå€¼ | **ç›˜é¢çŠ¶æ€ (Regime)** |
| **[Â§3 å½¢æ€åˆ†æ](#-4-åŸºç¡€å½¢æ€åˆ†æ-basic-patterns)** | å¥‡å¶ / å¤§å° / AC / å†·çƒ­ | **åå·®é¢„è­¦** |
| **[Â§4 å…³è”æŒ–æ˜](#-11-å·ç å…³è”è§„åˆ™æŒ–æ˜-association-rules)** | æå‡åº¦ / ç½®ä¿¡åº¦ / è·Ÿéšå¼ºåº¦ | **æ—¶åºå…³è” (Sequence)** |
| **[Â§5 ä½åºæ£®æ—](#-5-ä½åºæ£®æ—-stream-b-positional-focus)** | 20ç‚¹ä½ç‹¬ç«‹æ¨¡å‹é¢„æµ‹ | **4x5 çŸ©é˜µ / å‚ç›´åˆ†å¸ƒ** |
| **[Â§6 å…¨é‡æ·±åº¦](#-8-å…¨é‡å·ç æ·±åº¦åˆ†æ-full-80-numbers-detail)** | 80å·ç åˆ†åŒºåˆ†ç»„æ˜ç»† | **æ¦‚ç‡ / é—æ¼ / è¶‹åŠ¿** |
| **[Â§7 å®æˆ˜éªŒè¯](#-9-ç”¨æˆ·å®æˆ˜éªŒè¯-user-validation)** | è‡ªé€‰ç»„åˆ(Select2) / å•ç (SelectX) | **ä¿¡å¿ƒåˆ† / ä¸“å®¶è¯„ä»·** |
| **[Â§8 æŠ•èµ„å»ºè®®](#-10-é¦–å¸­æŠ•èµ„å»ºè®®-investment-strategy)** | å‡¯åˆ©å…¬å¼ä»“ä½åˆ†é… | **ä»“ä½æ¯”ä¾‹ / é£é™©æ§åˆ¶** |
| **[Â§9 æ¨¡å‹è´¡çŒ®](#-11-æ¨¡å‹æ¼”åŒ–ä¸ç‰¹å¾è´¡çŒ®-evolution--contribution)** | ç‰¹å¾è´¡çŒ®åº¦ / æƒé‡æ¼”åŒ– | **è·Ÿéšå¼ºåº¦åˆ†æ** |
"""

        # 2. åŒæµå…±æŒ¯ picks
        res_md = "| å…±æŒ¯å·ç  | å…¨å±€æ¦‚ç‡ | æ¨èç­‰çº§ | ä¸“å®¶å»ºè®® |\n|:---:|:---:|:---:|:---|\n"
        for r in result['resonance_picks'][:15]:
            advice = "é‡ç‚¹æ‰“å‡»" if r['prob'] > 0.28 else "ç¨³å¥é…ç½®"
            res_md += f"| **{r['num']:02d}** | `{r['prob']}` | {r['level']} | {advice} |\n"
            
        # 3. 20ç‚¹ä½è¯¦ç»†å‚ç›´åˆ†å¸ƒè¡¨ (ä¸€ä½ç½®ä¸€è¡Œ,å¤šç»´åº¦)
        pos_detail_md = "| ä½åº (Pos) | ğŸ”’ é¢„æµ‹ | ğŸ“ˆ æ¦‚ç‡ | â³ é—æ¼ | ğŸ¯ ä¿¡å¿ƒ | ğŸ” äº¤å‰éªŒè¯ | ğŸŒŸ è¯„çº§ |\n"
        pos_detail_md += "|:---:|:---:|:---:|:---:|:---:|:---|:---:|\n"
        for v in result['vertical_analysis']:
            pos_detail_md += f"| ç¬¬ {v['pos']:02d} ä½ | **{v['num']:02d}** | `{v['prob']}` | {v['gap']} | {v['score']} | {v['check']} | {v['rating']} |\n"

        # 3.1 äº”è±¡é™åˆ†å¸ƒæ ¼å¼åŒ– (README æ¢å¤)
        quad_md = "| è±¡é™ (16ç ) | æ ¸å¿ƒçƒ­ç‚¹ | èƒ½é‡å¯†åº¦ | è¯„çº§ |\n|:---:|:---|:---:|:---:|\n"
        for q in result['quadrants']:
            hot_str = " ".join([f"**{n:02d}**" for n in q['hot_nums']])
            quad_md += f"| {q['range']} | {hot_str} | `{q['avg_prob']}` | {q['rating']} |\n"

        # 4. å…³è”è§„åˆ™æŒ–æ˜ç»“æœ
        assoc_md = "#### ğŸ”— 4.1 äºŒé˜¶å…³è”è§„åˆ™ (Association)\n| å…³è”ç»„åˆ | æå‡åº¦ (Lift) | ç½®ä¿¡åº¦ (Conf) | å»ºè®® |\n|:---:|:---:|:---:|:---|\n"
        if result['assoc_rules']:
            for r in result['assoc_rules'][:10]:
                advice = "ğŸ”¥ å¼ºåŠ›å¸å¼•" if r['lift'] > 1.2 else "âœ… ç¨³å®šå…³è”"
                assoc_md += f"| {r['pair']} | `{r['lift']}` | `{r['conf']}` | {advice} |\n"
        else:
            assoc_md += "| - | - | - | æš‚æ— æ˜¾è‘—è§„åˆ™ |\n"

        # 5. è·Ÿéšå¼ºåº¦åˆ†æç»“æœ
        follower_md = "\n#### ğŸƒ 4.2 è·Ÿéšå¼ºåº¦åˆ†æ (Follower Strength)\n| è§¦å‘å·ç  | æ ¸å¿ƒè·Ÿéš (Top 3) | æœ€å¤§å¼ºåº¦ | å»ºè®® |\n|:---:|:---|:---:|:---:|\n"
        if result['follower_rules']:
            # ä»…æ˜¾ç¤ºæœ€è¿‘ä¸€æœŸå‡ºç°çš„å·ç çš„è·Ÿéšè§„åˆ™
            last_nums = result['last_patterns'].get('numbers', [])
            shown_count = 0
            for n in last_nums:
                if n in result['follower_rules']:
                    followers = result['follower_rules'][n]
                    f_str = " ".join([f"**{f['num']:02d}**({f['strength']})" for f in followers[:3]])
                    max_s = followers[0]['strength']
                    advice = "ğŸ”¥ å¼ºåŠ›è·Ÿéš" if max_s > 0.2 else "âœ… æ­£å¸¸è·Ÿéš"
                    follower_md += f"| {n:02d} | {f_str} | `{max_s}` | {advice} |\n"
                    shown_count += 1
            if shown_count == 0:
                follower_md += "| - | - | - | æš‚æ— è§¦å‘ |\n"
        else:
            follower_md += "| - | - | - | æš‚æ— è·Ÿéšæ•°æ® |\n"

        # 6. åˆ†åŒºå…¨é‡è¡¨ (1-20, 21-40, 41-60, 61-80)
        full_table_md = ""
        full_data = {row['num']: row for row in result['full_table']}
        for start_num in [1, 21, 41, 61]:
            end_num = start_num + 19
            full_table_md += f"\n#### ğŸ“ åˆ†åŒº {start_num} -{end_num}\n"
            full_table_md += "| å·ç  | æ¦‚ç‡ | é—æ¼ | å¾—åˆ† | è¶‹åŠ¿ | çŠ¶æ€ |\n|:---:|:---:|:---:|:---:|:---:|:---:|\n"
            for n in range(start_num, end_num + 1):
                row = full_data.get(n, {"prob": 0, "gap": 0, "score": 0, "trend": "-"})
                status = "ğŸ”¥" if row['score'] > 28 else "âœ¨" if row['score'] > 26 else "â¡ï¸"
                full_table_md += f"| {n:02d} | {row['prob']} | {row['gap']} | {row['score']} | {row['trend']} | {status} |\n"

        # 7. ç”¨æˆ·å®æˆ˜éªŒè¯ (Select Engine)
        select_md = ""
        if select_results:
            if select_results['select2']:
                select_md += "\n### ğŸ“‚ Select2 ç»„åˆè¯„ä¼°\n| ç»„åˆ | ç³»ç»Ÿä¿¡å¿ƒåˆ† | æ¨èåº¦ | ä¸“å®¶è¯„ä»· |\n|:---:|:---:|:---:|:---|\n"
                for s in select_results['select2']:
                    rec = "âœ…" if s['score'] > 26 else "âš ï¸"
                    comment = "æé«˜å…±æŒ¯,å»ºè®®é‡ä»“" if s['score'] > 28 else "æ¦‚ç‡å ä¼˜,å»ºè®®é…ç½®" if s['score'] > 26 else "æ•°æ®ä¸€èˆ¬,è°¨æ…å‚è€ƒ"
                    select_md += f"| **{s['nums']}** | `{s['score']}` | {rec} | {comment} |\n"
            
            if select_results['selectX']:
                select_md += "\n### ğŸ“‚ SelectX å·ç è¯„ä¼°\n| å·ç  | ç³»ç»Ÿæ¦‚ç‡ | ä¿¡å¿ƒåˆ† | è¯„ä»· | å»ºè®® |\n|:---:|:---:|:---:|:---:|:---:|\n"
                for s in select_results['selectX']:
                    star = "ğŸŒŸ" if s['score'] > 28 else "âœ¨" if s['score'] > 26 else "âšª"
                    advice = "æ ¸å¿ƒèƒ†ç " if s['score'] > 28 else "è¾…åŠ©å‚è€ƒ"
                    select_md += f"| **{s['num']:02d}** | `{s['prob']}` | `{s['score']}` | {star} | {advice} |\n"

        # å®¡è®¡æ—¥å¿—æ ¼å¼åŒ–
        audit_md = "\n".join([f"- {log}" for log in self.data_engine.audit_log])
        
        # å‡¯åˆ©å…¬å¼æ ¼å¼åŒ–
        kelly = result['kelly_advice']
        kelly_md = "| å·ç  | é¢„æµ‹æ¦‚ç‡ | å»ºè®®ä»“ä½ (Kelly) | é£é™©çº§åˆ« |\n|:---:|:---:|:---:|:---:|\n"
        for a in kelly['advice']:
            kelly_md += f"| **{a['num']:02d}** | `{a['prob']}` | **{a['sizing']}** | {a['level']} |\n"
        kelly_md += f"\n> **ç­–ç•¥ç»¼è¿°**: {kelly['summary']}"

        # å…«åˆ†åŒºæ ¼å¼åŒ–
        zone_md = "| åˆ†åŒº | çƒ­ç‚¹å·ç  | èƒ½é‡å¯†åº¦ | è¯„çº§ |\n|:---:|:---|:---:|:---:|\n"
        for z in result['zones']:
            hot_str = " ".join([f"**{n:02d}**" for n in z['hot_nums']])
            zone_md += f"| {z['range']} | {hot_str} | `{z['avg_prob']}` | {z['rating']} |\n"

        # æƒé‡å±•ç¤º
        w = SupremeConfig.FUSION_WEIGHTS
        weight_md = f"| RF/MLP (A+C) | GBDT (XGB/LGB) | TCN (D) | ARIMA (E) |\n|:---:|:---:|:---:|:---:|\n| `{w['rf_mlp']:.2f}` | `{w['gbdt']:.2f}` | `{w['tcn']:.2f}` | `{w['arima']:.2f}` |"

        # 11. æ¨¡å‹è´¡çŒ®åº¦ä¸æ¼”åŒ–åˆ†æ (New)
        importance_data = self.global_ml.get_importance_report()
        imp_md = "| ç‰¹å¾åç§° | è´¡çŒ®åº¦ (Weight) | çŠ¶æ€ | è¯„ä»· |\n|:---:|:---:|:---:|:---|\n"
        for imp in importance_data[:8]:  # æ˜¾ç¤º Top 8
            status = "ğŸ”¥ æ ¸å¿ƒ" if imp['importance'] > 0.1 else "âœ… æœ‰æ•ˆ"
            comment = "æ–°å¼•å…¥ç‰¹å¾" if imp['feature'] == "Follower_Strength" else "åŸºç¡€ç‰¹å¾"
            imp_md += f"| {imp['feature']} | `{imp['importance']}` | {status} | {comment} |\n"
            
        # è¯»å–è°ƒä¼˜å†å²è¶‹åŠ¿
        history_path = SupremeConfig.BASE_DIR / "data" / "tuner_history.json"
        evolution_md = "| æ—¶é—´æˆ³ | å‘½ä¸­ç‡ | RF/MLP | GBDT | TCN | ARIMA |\n|:---:|:---:|:---:|:---:|:---:|:---:|\n"
        if history_path.exists():
            try:
                with open(history_path, 'r', encoding='utf-8') as f:
                    t_hist = json.load(f)
                for h in t_hist[-5:]:  # æœ€è¿‘ 5 æ¬¡æ¼”åŒ–
                    w_h = h.get('weights', {})
                    evolution_md += f"| {h['timestamp'][5:16]} | `{h['best_value']}` | {w_h.get('rf_mlp',0)} | {w_h.get('gbdt',0)} | {w_h.get('tcn',0)} | {w_h.get('arima',0)} |\n"
            except Exception:
                evolution_md += "| - | - | - | - | - | å†å²è¯»å–å¤±è´¥ |\n"
        else:
            evolution_md += "| - | - | - | - | - | åˆå§‹è¿è¡Œæ— å†å² |\n"

        content = f"""# ğŸ”¬ GUCP-X å…¨ç»´é‡åŒ–æ·±åº¦ç ”æŠ¥ â€“ é¦–å¸­æ‰§è¡Œç‰ˆ
---
> **ç”Ÿæˆæ—¶é—´**: `{datetime.now().strftime('%Y-%m-%d %H:%M:%S')}`
> **é¦–å¸­ç§‘å­¦å®¶**: `Chief Quant Scientist` | **éªŒè¯çŠ¶æ€**: `STRICT_VALIDATED` | **ç‰ˆæœ¬**: `{SupremeConfig.VERSION}`

## ğŸ“‹ æŠ¥å‘Šå¯¼èˆª (Report Navigation)
{nav_md}

## ğŸ¯ 0. é¦–å¸­æ‰§è¡Œç»¼è¿° (Executive Summary)
> **æ ¸å¿ƒæ´å¯Ÿ**:æ¨¡å‹åŸºäº {len(self.data_engine.history)} æœŸå†å²æ•°æ®æ·±åº¦è®­ç»ƒ.äº”æµç³»ç»Ÿ (A+B+C+D+E) å·²å…¨é¢ä¸Šçº¿.
> **å‚æ•°è°ƒä¼˜**: å·²å®Œæˆè‡ªåŠ¨è°ƒä¼˜ (AutoTuner),å½“å‰æœ€ä½³èåˆæƒé‡:
{weight_md}
> **æœ¬æœŸåˆ¤è¯**: **[Global]** å®è§‚æœ‰åº,**[Positional]** ç»†èŠ‚ä¸°å¯Œ.å»ºè®®é‡‡ç”¨ **å…±æŒ¯ä¼˜å…ˆ,é˜²å®ˆåå‡»** ç­–ç•¥.

## ğŸŒŠ 1. åŒæµæ ¸å¿ƒ (Dual-Stream Intelligence)
> **æ¶æ„é€»è¾‘**: æœ¬ç³»ç»Ÿæ·±åº¦èåˆ **[å…¨å±€éšæœºæ£®æ—] (Stream A)**, **[ä½åºéšæœºæ£®æ—] (Stream B)**, **[MLP ç¥ç»ç½‘ç»œ] (Stream C)**, **[TCN æ—¶åºå·ç§¯] (Stream D)** ä¸ **[ARIMA æ—¶é—´åºåˆ—] (Stream E)**.

### ğŸ§  1.0 æ··åˆå¼•æ“çŠ¶æ€ (Hybrid Engine Status)
| å¼•æ“æµ | æ¨¡å‹ç®—æ³• | æ ¸å¿ƒç‰¹å¾ | çŠ¶æ€ |
|:---:|:---|:---|:---:|
| **Stream A** | Random Forest | å…¨å±€é¢‘ç‡, é—æ¼è¡°å‡ | âœ… Active |
| **Stream B** | Positional Forest | 20ç‚¹ä½ç‹¬ç«‹åºåˆ— | âœ… Active |
| **Stream C** | MLP Neural Net | éçº¿æ€§ç‰©ç†åœº, è·¨æœŸç›¸å…³ | âœ… Active |
| **Stream D** | TCN Network | æ—¶åºé•¿ç¨‹ä¾èµ–, æ‰©å¼ å·ç§¯ | âœ… Active |
| **Stream E** | ARIMA / GBDT | å°æ ·æœ¬è¶‹åŠ¿, æ¢¯åº¦æå‡ | âœ… Active |

### ğŸ’ 1.1 åŒæµå…±æŒ¯æ¨è (Resonance Picks)
{res_md}

## ğŸ›¡ï¸ 2. æ•°æ®è´¨é‡å®¡è®¡ (Data Audit)
{audit_md}

## ğŸŒ 3. å¸‚åœºç¯å¢ƒæ„ŸçŸ¥ (Market Regime)
> **æ·±åº¦æ„ŸçŸ¥**: åŸºäºè¿‘æœŸå’Œå€¼è¶‹åŠ¿æ–œç‡, æ³¢åŠ¨ç‡åŠ **Shannon ç†µ (Entropy)** åŠ¨æ€è°ƒæ•´æ¨¡å‹è§‚å¯Ÿçª—å£.

| æŒ‡æ ‡ | å½“å‰å€¼ | å‚è€ƒèŒƒå›´ | çŠ¶æ€ |
|:---:|:---:|:---:|:---:|
| **ç›˜é¢çŠ¶æ€** | `{regime['status']}` | - | - |
| **è¶‹åŠ¿æ–œç‡** | `{regime['slope']}` | >2.5 æˆ– <-2.5 | {"ğŸ“ˆ" if regime['slope'] > 0 else "ğŸ“‰" if regime['slope'] < 0 else "âš–ï¸"} |
| **æ³¢åŠ¨ç‡** | `{regime['volatility']}` | <0.04(ç¨³) >0.07(ä¹±) | {"ğŸŒªï¸" if regime['volatility'] > 0.07 else "âš–ï¸"} |
| **ç›˜é¢ç†µå€¼** | `{regime['entropy']}` | <5.8(é›†) >6.1(æ•£) | {"ğŸ§©" if regime['entropy'] < 5.8 else "ğŸŒªï¸"} |
| **æ¨èçª—å£** | `{regime['recommended_window']}` | 8-15 | **è‡ªé€‚åº”åŒæ­¥** |

## ğŸ“Š 4. åŸºç¡€å½¢æ€åˆ†æ (Basic Patterns)
| æŒ‡æ ‡ | æ•°å€¼ | ç†è®ºå‚è€ƒ | çŠ¶æ€ |
|:---|:---:|:---:|:---|
| **å¥‡å¶æ¯”** | `{patterns['odd_even']}` | 10:10 | {"ğŸŸ¢ å¹³è¡¡" if "10:10" in patterns['odd_even'] else "âš ï¸ åå·®"}
| **å¤§å°æ¯”** | `{patterns['big_small']}` | 10:10 | {"ğŸŸ¢ å¹³è¡¡" if "10:10" in patterns['big_small'] else "âš ï¸ åå·®"}
| **è´¨åˆæ¯”** | `{patterns['prime_composite']}` | ~5:15 | -
| **AC å€¼** | `{patterns.get('ac', 'N/A')}` | > 65 | {"ğŸ”¥ å¤æ‚" if patterns.get('ac', 0) > 75 else "âš–ï¸ æ­£å¸¸"}
| **æœ€å¤§è¿å·** | `{patterns.get('max_consecutive', 'N/A')}` | ~3-4 | {"ğŸ”¥ èµ°çƒ­" if patterns.get('max_consecutive', 0) > 4 else "âš–ï¸ æ­£å¸¸"}
| **è¿å·ç»„æ•°** | `{patterns.get('consecutive_groups', 'N/A')}` | ~5 | -
| **é‡å·/é‚»å·**| `{patterns['repeat']}/{patterns.get('neighbor', 'N/A')}` | ~6/12 | -
| **å†·çƒ­æ¸©æ¯”** | `{patterns.get('chw', 'N/A')}` | 4:12:4 | (çƒ­:æ¸©:å†·)
| **å’Œå€¼** | `{patterns['sum']}` | 810 | {"ğŸ”½ åä½" if patterns['sum'] < 810 else "ğŸ”¼ åé«˜"}
| **è·¨åº¦** | `{patterns['span']}` | ~73 | -
| **å°¾æ•°åˆ†å¸ƒ** | `{patterns.get('tails', 'N/A')}` | (0-9) | å‡å€¼:2

## ğŸ”— 4. å…³è”æŒ–æ˜ (Association & Follower)
> **æŒ–æ˜é€»è¾‘**: ç»“åˆäºŒé˜¶å…³è”è§„åˆ™ä¸æ—¶åºè·Ÿéšå¼ºåº¦,è¯†åˆ«å·ç é—´çš„æ·±å±‚ç‰µå¼•åŠ›.

{assoc_md}
{follower_md}

## ğŸ“ 5. ä½åºæ£®æ— (Stream B: Positional Focus)
> **åˆ†æé€»è¾‘**: é’ˆå¯¹ 20 ä¸ªå‡ºçƒä½åºåˆ†åˆ«å»ºç«‹ç‹¬ç«‹çš„éšæœºæ£®æ—æ¨¡å‹,æ•æ‰ä½ç½®ç‰¹æœ‰çš„ç‰©ç†æƒ¯æ€§ä¸åºåˆ—è§„å¾‹.

### ğŸ“‹ 5.1 ä½åºå…¨ç»´åº¦æ·±åº¦è§£æ (Full Positional Analysis)
{pos_detail_md}

### ğŸ—ºï¸ 5.2 äº”è±¡é™èƒ½é‡åˆ†å¸ƒ (Quadrants)
> **åˆ†æé€»è¾‘**: å°† 80 ä¸ªå·ç åˆ’åˆ†ä¸º 5 ä¸ªå¤§åŒº(æ¯åŒº 16 ç ),åˆ†æå¤§å°ºåº¦çš„å·ç èƒ½é‡èšé›†æ•ˆåº”.
{quad_md}

## ğŸ—ºï¸ 6. æ¦‚ç‡åˆ†å¸ƒçƒ­ç‚¹ (Zone Analysis)
{zone_md}

## ğŸ¯ 7. æ ¸å¿ƒé¢„æµ‹è¾“å‡º (Core Targets)
### ğŸ“ æ ¸å¿ƒ 20 ç‚¹ä½ (Core 20)
`{core_20_str}`

### ğŸ›¡ï¸ æ™ºèƒ½æ‰©å±•å¤§åº• (Smart Pool - {len(smart_pool)}ç )
`{pool_str}`

## ğŸ”¢ 8. å…¨é‡å·ç æ·±åº¦åˆ†æ (Full 80 Numbers Detail)
{full_table_md}

## ğŸ“‚ 9. ç”¨æˆ·å®æˆ˜éªŒè¯ (User Validation)
{select_md}

## ğŸ“‰ 10. é¦–å¸­æŠ•èµ„å»ºè®® (Investment Strategy)
> **å†³ç­–é€»è¾‘**: åŸºäº **å‡¯åˆ©å…¬å¼ (Kelly Criterion)** è®¡ç®—æœ€ä¼˜ä»“ä½åˆ†é…,å¹³è¡¡é¢„æœŸæ”¶ç›Šä¸ç ´äº§é£é™©.
{kelly_md}

---
{validation_md}

## ğŸ“ˆ 11. æ¨¡å‹æ¼”åŒ–ä¸ç‰¹å¾è´¡çŒ® (Evolution & Contribution)
> **åˆ†æé€»è¾‘**: é€šè¿‡ **SHAP/Permutation Importance** åŸç†é‡åŒ–å„ç‰¹å¾å¯¹é¢„æµ‹ç»“æœçš„è¾¹é™…è´¡çŒ®,å¹¶è¿½è¸ª **AutoTuner** çš„èåˆæƒé‡æ¼”åŒ–è·¯å¾„.

#### ğŸ“Š 11.1 ç‰¹å¾è´¡çŒ®åº¦æ’è¡Œ (Top Feature Importance)
{imp_md}
> **ç»“è®º**: è‹¥ `Follower_Strength` è¿›å…¥ Top 5,è¯´æ˜å½“å‰ç›˜é¢å—å·ç é—´æ—¶åºå¸å¼•åŠ›å½±å“æ˜¾è‘—.

#### ğŸ”„ 11.2 èåˆæƒé‡æ¼”åŒ–è¶‹åŠ¿ (Weight Evolution)
{evolution_md}
> **ç­–ç•¥å«ä¹‰**: æƒé‡å‘æŸä¸€æµæ´¾å€¾æ–œ(å¦‚ TCN æˆ– GBDT)åæ˜ äº†å¸‚åœºè¿‘æœŸçš„æ³¢åŠ¨æ¨¡å¼å˜åŒ–.

## ğŸ”¬ 12. ç‰©ç†åœºæ·±å±‚ç‰¹å¾ (Quant Insights)
- **å…±æŒ¯é¢‘ç‡**: {result['resonance_count']} (åŒæ¨¡å‹ä¸€è‡´æ€§æŒ‡æ ‡)
- **è‡ªé€‚åº”çª—å£**: {regime['recommended_window']} (æ ¹æ®å¸‚åœºçŠ¶æ€è‡ªåŠ¨è°ƒèŠ‚)
- **ç‰¹å¾ç»´åº¦**: 13 ç»´æ·±åº¦ç‰¹å¾ (æ–°å¢è·¨æœŸç›¸å…³æ€§, é—æ¼è¡°å‡, å°¾æ•°çƒ­åº¦)
- **ç‰©ç†åœºç‰¹å¾**: åŒ…å« Hurst, Entropy, Volatility ç­‰éçº¿æ€§æŒ‡æ ‡
- **Hurst æŒ‡æ•°**: `{patterns.get('hurst', '0.5')}` (åºåˆ—è®°å¿†å¼ºåº¦)
- **å…³è”è§„åˆ™**: äºŒé˜¶å…³è”æŒ–æ˜ (Top 15 ç»„åˆ)
- **æ—¶åºæ¨¡å‹**: TCN (Temporal Convolutional Network) å·²é›†æˆè‡³ Stream D
- **è·Ÿéšå¼ºåº¦**: æ•æ‰ A->B çš„æ—¶åºè·Ÿéšè§„å¾‹

---
*Generated by Antigravity Quant Engine (Supreme Gold Unified Edition)*
"""
        with open(report_path, "w", encoding="utf-8") as f:
            f.write(content)
        
        self.logger.info(f"âœ¨ ç ”æŠ¥å·²ç”Ÿæˆ: {report_path}")
        print(f"\n[SUCCESS] ç ”æŠ¥å·²å°±ç»ª: {report_path}")

if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="GUCP-X Supreme Unified System")
    parser.add_argument("--no-backtest", action="store_true", help="è·³è¿‡å›æµ‹æ­¥éª¤")
    parser.add_argument("--no-persist", action="store_true", help="è·³è¿‡æ¨¡å‹æŒä¹…åŒ–")
    parser.add_argument("--no-tune", action="store_true", help="è·³è¿‡è‡ªåŠ¨è°ƒä¼˜")
    parser.add_argument("--incremental", action="store_true", help="å¯ç”¨å¢é‡è®­ç»ƒæ¨¡å¼")
    parser.add_argument("--trials", type=int, help="è®¾ç½® Optuna è°ƒä¼˜æ¬¡æ•°")
    parser.add_argument("--backtest-periods", type=int, help="è®¾ç½®å›æµ‹å‘¨æœŸæ•°")
    args = parser.parse_args()
    
    # è¦†ç›–é…ç½®
    if args.trials:
        SupremeConfig.AUTO_TUNE_TRIALS = args.trials
    if args.backtest_periods:
        SupremeConfig.VALIDATION_SIZE = args.backtest_periods

    manager = SupremeManager()
    manager.run_production_pipeline(
        run_backtest=not args.no_backtest,
        persist_models=not args.no_persist,
        auto_tune=not args.no_tune,
        incremental=args.incremental
    )