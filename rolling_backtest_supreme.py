"""
GUCP-X Supreme Rolling Backtest Engine
é¦–å¸­å…¨ç»´é‡åŒ–ç§‘å­¦å®¶ä¸“ç”¨ - æ»šåŠ¨å›æµ‹ç³»ç»Ÿ
æ­¤è„šæœ¬ä¸ºç‹¬ç«‹çš„é«˜çº§å¤–æŒ‚ï¼Œç”¨äºæ‰§è¡Œä¸¥æ ¼çš„é€æœŸæ»šåŠ¨éªŒè¯ã€‚
"""

import sys
import os
import json
import time
import logging
import pandas as pd
import numpy as np
from datetime import datetime
from pathlib import Path
from tqdm import tqdm
import torch

# å¯¼å…¥ä¸»ç¨‹åºæ ¸å¿ƒå¼•æ“
try:
    # å‡è®¾ä¸»ç¨‹åºåœ¨åŒä¸€ç›®å½•ä¸‹ï¼Œæˆ–è€…åœ¨ PYTHONPATH ä¸­
    import gucp_x_supreme_unified as supreme
except ImportError:
    # å°è¯•æ·»åŠ å½“å‰ç›®å½•å¹¶é‡è¯•
    sys.path.append(os.getcwd())
    import gucp_x_supreme_unified as supreme

class SupremeRollingBacktester:
    def __init__(self, validation_size=300, retrain_interval=1):
        self.logger = self._setup_logger()
        self.validation_size = validation_size
        self.retrain_interval = retrain_interval # æ¯éš” N æœŸé‡è®­ä¸€æ¬¡
        
        # åˆå§‹åŒ–æ ¸å¿ƒå¼•æ“
        supreme.SupremeConfig.init_environment()
        self.data_engine = supreme.DataEngine()
        self.results_dir = supreme.SupremeConfig.REPORT_DIR / "rolling_backtests"
        self.results_dir.mkdir(exist_ok=True)
        
        # åˆå§‹åŒ–æ¨¡å‹ç®¡ç†å™¨
        self.manager = supreme.SupremeManager()
        
        # åŠ è½½å…¨é‡æ•°æ®
        self.full_history = self.data_engine.history
        self._validate_data()

    def _setup_logger(self):
        logger = logging.getLogger("RollingBacktest")
        logger.setLevel(logging.INFO)
        formatter = logging.Formatter('%(asctime)s | %(levelname)-8s | %(message)s')
        
        # æ§åˆ¶å°è¾“å‡º
        ch = logging.StreamHandler()
        ch.setFormatter(formatter)
        logger.addHandler(ch)
        return logger

    def _validate_data(self):
        total = len(self.full_history)
        if total < self.validation_size + 100:
            raise ValueError(f"æ•°æ®é‡ä¸è¶³ï¼æ€»å…± {total} æœŸï¼Œè¦æ±‚éªŒè¯é›† {self.validation_size} æœŸã€‚")
        self.logger.info(f"ğŸ“Š æ•°æ®åŠ è½½å®Œæˆ: æ€»è®¡ {total} æœŸï¼ŒéªŒè¯é›† {self.validation_size} æœŸ")

    def run(self):
        """æ‰§è¡Œæ»šåŠ¨å›æµ‹ä¸»å¾ªç¯"""
        total = len(self.full_history)
        start_idx = total - self.validation_size
        
        # åˆå§‹è®­ç»ƒé›†
        current_history = self.full_history[:start_idx]
        validation_set = self.full_history[start_idx:]
        
        results = []
        pbar = tqdm(total=len(validation_set), desc="ğŸ”„ Rolling Backtest")
        
        # è®°å½•ç´¯ç§¯æ”¶ç›Š
        cum_pnl = 0
        
        for i, target_draw in enumerate(validation_set):
            period = target_draw['period']
            
            # 1. åŠ¨æ€è®­ç»ƒ (Periodic Retraining)
            if i % self.retrain_interval == 0:
                self.logger.info(f"ğŸ§  [Period {period}] Re-training models... (Train Size: {len(current_history)})")
                
                # å¼ºåˆ¶é‡è®­æ¨¡å¼ mode='rolling'ï¼Œé¿å…è¯»å–éæ­¤è½®çš„ç¼“å­˜
                # æ³¨æ„ï¼šè¿™é‡Œæˆ‘ä»¬è°ƒç”¨ train_or_load ä½†ä¼ å…¥ force=True æ¥ç¡®ä¿ä½¿ç”¨æœ€æ–°çš„ current_history
                # ä¸ºäº†æ•ˆç‡ï¼Œæˆ‘ä»¬åªé‡è®­æ ¸å¿ƒæ¨¡å‹ï¼šGlobal (MLP/RF) å’Œ Positional
                data_time = time.time() # ä¼ªé€ æ—¶é—´æˆ³å¼ºåˆ¶æ›´æ–°
                
                # A. å¸‚åœºæ„ŸçŸ¥è·å–æ¨èçª—å£
                try:
                    regime_info = supreme.MarketEngine.analyze_regime(current_history)
                    rec_window = regime_info['recommended_window']
                except:
                    rec_window = 12
                
                self.manager.global_ml.train_or_load(current_history, data_time, window=rec_window, mode='rolling', force=True)
                self.manager.pos_ml.train_or_load(current_history, data_time, mode='rolling', force=True)
                
                # TCN è®­ç»ƒè¾ƒæ…¢ï¼Œå¯ä»¥é€‰æ‹©ä¸æ¯æœŸé‡é¡ºï¼Œæˆ–è€…æ¯ 10 æœŸé‡è®­ä¸€æ¬¡
                if i % 10 == 0:
                     self.manager.tcn_engine.train_or_load(current_history, data_time, mode='rolling', force=True)
            
            # 2. ç”Ÿæˆé¢„æµ‹
            # ä½¿ç”¨æœ€æ–°çš„ current_history è¿›è¡Œé¢„æµ‹
            probs_dict = self.manager.global_ml.predict(current_history, window=12) # çª—å£å¯ä»¥åŠ¨æ€åŒ–
            pos_preds = self.manager.pos_ml.predict(current_history)
            tcn_probs = self.manager.tcn_engine.predict(current_history)
            arima_probs = self.manager.arima_engine.predict(current_history)
            
            # 3. æ ¸å¿ƒèåˆç”Ÿæˆ Smart Pool
            pool_info = supreme.KernelEngine.generate_smart_pool(
                probs_dict, pos_preds, current_history,
                tcn_probs=tcn_probs,
                arima_probs=arima_probs,
                loaded_core_points=self.data_engine.core_points
            )
            
            smart_pool = pool_info['smart_pool']
            core_20 = pool_info['core_20']
            
            # 4. éªŒè¯ç»“æœ
            real_nums = set(target_draw['sorted'])
            pool_hits = len(real_nums.intersection(smart_pool))
            core_hits = len(real_nums.intersection(core_20))
            
            # ç®€å• PnL è®¡ç®— (å‡è®¾æ¯æ¬¡æŠ•å…¥ Smart Pool å¤§å°ï¼Œä¸­ 1 ä¸ªå¾— 1 åˆ† - ä»…ä½œç¤ºæ„)
            # å®é™… PnL åº”è¯¥æ›´å¤æ‚ï¼Œè¿™é‡Œç”¨ (å‘½ä¸­æ•° * 4.6 - æŠ•å…¥æœ¬é‡‘) ç®€æ˜“æ¨¡æ‹Ÿé€‰äº”åŠæ ¼çº¿
            # å‡è®¾æ™ºèƒ½åº•æ˜¯é€‰åç©æ³•çš„å¤§åº•
            pnl_step = pool_hits - (len(smart_pool) * 0.25) 
            cum_pnl += pnl_step
            
            # 5. è®°å½•æ—¥å¿—
            log_entry = {
                "period": period,
                "date": target_draw['date'],
                "train_size": len(current_history),
                "pool_size": len(smart_pool),
                "pool_hits": pool_hits,
                "core_hits": core_hits,
                "hit_rate_pool": round(pool_hits / len(smart_pool), 4),
                "core_vals": "-".join([f"{n:02d}" for n in core_20]),
                "real_vals": "-".join([f"{n:02d}" for n in sorted(list(real_nums))]),
                "pnl_step": round(pnl_step, 2),
                "cum_pnl": round(cum_pnl, 2),
                "regime": pool_info.get('regime', {}).get('status', 'N/A')
            }
            results.append(log_entry)
            
            # æ§åˆ¶å°ç®€æŠ¥
            if i % 10 == 0 or pool_hits >= 10: # é«˜å…‰æ—¶åˆ»æˆ–å®šæœŸè¾“å‡º
                self.logger.info(f"ğŸ“ Period {period} | Core Hits: {core_hits}/20 | Pool Hits: {pool_hits}/{len(smart_pool)} | Cumulative PnL: {cum_pnl:.1f}")
            
            # 6. ä¸è¾¾æ ‡åé¦ˆ (Adaptive Logic Demo)
            if core_hits < 3:
                self.logger.warning(f"âš ï¸ æ ¸å¿ƒå‘½ä¸­åä½ ({core_hits})ï¼Œä¸‹ä¸€è½®å°†ä¿æŒé‡è®­ä»¥è°ƒæ•´çŠ¶æ€ã€‚")
                # è¿™é‡Œå¯ä»¥åŠ å…¥é€»è¾‘ï¼Œä¾‹å¦‚ä¸‹ä¸€æœŸå¼ºåˆ¶é‡è®­ (å¦‚æœ retrain_interval > 1)
            
            # 7. æ»šåŠ¨ï¼šå°†æœ¬æœŸçœŸå€¼åŠ å…¥å†å²ï¼Œç”¨äºä¸‹ä¸€æœŸè®­ç»ƒ/ç‰¹å¾æå–
            current_history.append(target_draw)
            
            # 8. å®šæœŸä¿å­˜ä¸­é—´ç»“æœ
            if i % 10 == 0:
                self._save_results(results, is_final=False)
                
            pbar.update(1)
            
        pbar.close()
        self._save_results(results, is_final=True)
        self.logger.info("âœ… æ»šåŠ¨å›æµ‹å…¨æµç¨‹ç»“æŸã€‚")

    def _save_results(self, results, is_final=False):
        df = pd.DataFrame(results)
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        suffix = "FINAL" if is_final else "checkpoint"
        filename = f"rolling_backtest_{self.validation_size}periods_{suffix}.csv"
        path = self.results_dir / filename
        df.to_csv(path, index=False, encoding='utf-8-sig')
        if is_final:
            self.logger.info(f"ğŸ“„ æœ€ç»ˆæŠ¥å‘Šå·²ç”Ÿæˆ: {path}")
            
            # ç”Ÿæˆç®€å•çš„ç»Ÿè®¡æ‘˜è¦
            summary = {
                "total_periods": len(results),
                "avg_pool_hits": df['pool_hits'].mean(),
                "avg_core_hits": df['core_hits'].mean(),
                "total_pnl": df['pnl_step'].sum(),
                "win_rate_pool_gt_10": (df['pool_hits'] >= 10).mean()
            }
            summary_path = self.results_dir / f"summary_{timestamp}.json"
            with open(summary_path, 'w', encoding='utf-8') as f:
                json.dump(summary, f, indent=4)

if __name__ == "__main__":
    # è§£æå‘½ä»¤è¡Œå‚æ•° (å¯é€‰)
    import argparse
    parser = argparse.ArgumentParser()
    parser.add_argument("--periods", type=int, default=300, help="éªŒè¯é›†æœŸæ•° (æœ€æ–° N æœŸ)")
    parser.add_argument("--interval", type=int, default=1, help="é‡è®­é—´éš” (æ¯ N æœŸé‡è®­ä¸€æ¬¡)")
    args = parser.parse_args()
    
    print(f"ğŸš€ å¯åŠ¨æ»šåŠ¨å›æµ‹ | éªŒè¯æœŸæ•°: {args.periods} | é‡è®­é—´éš”: {args.interval}")
    
    try:
        tester = SupremeRollingBacktester(validation_size=args.periods, retrain_interval=args.interval)
        tester.run()
    except KeyboardInterrupt:
        print("\nğŸ›‘ ç”¨æˆ·ä¸­æ–­æµ‹è¯•")
    except Exception as e:
        print(f"\nâŒ å‘ç”Ÿä¸¥é‡é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()
